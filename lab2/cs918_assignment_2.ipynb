{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a Jupyter notebook for the coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, donâ€™t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "You may import more packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from os.path import join\n",
    "from utils.file import read_file_lines_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "test_set_names: tuple[str, ...] = (\n",
    "    'twitter-test1.txt', \n",
    "    'twitter-test2.txt', \n",
    "    'twitter-test3.txt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts: tuple[str, ...] = ('positive', 'negative', 'neutral')\n",
    "\n",
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(test_set_name_: str) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Read in the test_set and return a dictionary\n",
    "    :param test_set_name_: str, the file name of the test_set to compare\n",
    "    \"\"\"\n",
    "    id_gts: dict[str, str] = {}\n",
    "    lines = read_file_lines_from(f'data/{test_set_name_}')\n",
    "    for line in lines:\n",
    "        fields = line.split('\\t')\n",
    "        tweet_id = fields[0]\n",
    "        gt = fields[1]\n",
    "\n",
    "        id_gts[tweet_id] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_predicts_: dict[str, str], test_set_name_: str) -> None:\n",
    "    \"\"\"\n",
    "    print the confusion matrix of {'positive', 'negative'} between predicts and test_set\n",
    "    :param id_predicts_: a dictionary of predictions formatted as {<tweet_id>:<sentiment>, ... }\n",
    "    :param test_set_name_: str, the file name of the test_set to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    \"\"\"\n",
    "    id_gts: dict[str, str] = read_test(test_set_name_)\n",
    "\n",
    "    # FIXME: dead code.\n",
    "    # gts = []\n",
    "    # for m, c1 in id_gts.items():\n",
    "    #     if c1 not in gts:\n",
    "    #         gts.append(c1)\n",
    "\n",
    "    conf: dict[str, dict[str, int]] = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweet_id, gt in id_gts.items():\n",
    "        if tweet_id in id_predicts_:\n",
    "            pred = id_predicts_[tweet_id]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print(f\"{conf[c1][c2] / float(sum(conf[c1].values())):.3f}     \", end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "def evaluate(id_predicts_: dict[str, str], test_set_name_: str, classifier_):\n",
    "    \"\"\"\n",
    "    print the macro-F1 score of {'positive', 'negative'} between predicts and test_set\n",
    "    :param id_predicts_: a dictionary of predictions formatted as {<tweet_id>:<sentiment>, ... }\n",
    "    :param test_set_name_: str, the file name of the test_set to compare\n",
    "    :param classifier_: str, the name of the classifier\n",
    "    \"\"\"\n",
    "    id_gts: dict[str, str] = read_test(test_set_name_)\n",
    "\n",
    "    acc_by_class: dict[str, dict[str, int]] = {}\n",
    "    for gt in gts:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    ok = 0\n",
    "    for tweet_id, gt in id_gts.items():\n",
    "        if tweet_id in id_predicts_:\n",
    "            pred = id_predicts_[tweet_id]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    cat_count = 0\n",
    "    item_count = 0\n",
    "    macro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "    micro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "    sem_eval_macro: dict[str, int] = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    micro_tp = 0\n",
    "    micro_fp = 0\n",
    "    micro_tn = 0\n",
    "    micro_fn = 0\n",
    "\n",
    "    cat_f1s: dict[str, int] = {}\n",
    "\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        cat_count += 1\n",
    "\n",
    "        micro_tp += acc['tp']\n",
    "        micro_fp += acc['fp']\n",
    "        micro_tn += acc['tn']\n",
    "        micro_fn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        cat_f1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            sem_eval_macro['p'] += p\n",
    "            sem_eval_macro['r'] += r\n",
    "            sem_eval_macro['f1'] += f1\n",
    "\n",
    "        item_count += n\n",
    "\n",
    "    micro['p'] = float(micro_tp) / float(micro_tp + micro_fp)\n",
    "    micro['r'] = float(micro_tp) / float(micro_tp + micro_fn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    sem_eval_macro_f1 = sem_eval_macro['f1'] / 2\n",
    "\n",
    "    print(f\"{test_set_name_} ({classifier_}): {sem_eval_macro_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "data: dict[str, list[...]] = {}\n",
    "tweet_ids: dict[str, list[...]] = {}\n",
    "tweet_gts: dict[str, list[...]] = {}\n",
    "tweets: dict[str, list[...]] = {}\n",
    "\n",
    "for dataset in ('twitter-training-data.txt',) + test_set_names:\n",
    "    data[dataset] = []\n",
    "    tweets[dataset] = []\n",
    "    tweet_ids[dataset] = []\n",
    "    tweet_gts[dataset] = []\n",
    "\n",
    "    # write code to read in the datasets here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.\n",
    "for classifier in ('svm', '<classifier-2-name>', '<classifier-3-name>',):\n",
    "    for features in ('bow', '<feature-2-name>',):\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'svm':\n",
    "            # write the svm classifier here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == '<classifier-2-name>':\n",
    "            # write the classifier 2 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == '<classifier-3-name>':\n",
    "            # write the classifier 3 here\n",
    "            print('Training ' + classifier)\n",
    "        elif classifier == 'LSTM':\n",
    "            # write the LSTM classifier here\n",
    "            if features == 'bow':\n",
    "                continue\n",
    "            print('Training ' + classifier)\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        # Prediction performance of the classifiers\n",
    "        for test_set_name in test_set_names:\n",
    "            id_predicts = {}\n",
    "            # write the prediction and evaluation code here\n",
    "\n",
    "            test_set_path = join('semeval-tweets', test_set_name)\n",
    "            evaluate(id_predicts, test_set_path, features + '-' + classifier)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
