{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a Jupyter notebook for the coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, don’t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from os import path, getcwd, listdir, makedirs\n",
    "\n",
    "\n",
    "def read_file_lines_from(file_path: str, /) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Read lines from a file and yield each line as a string.\n",
    "    The path to the file is relative to the current working directory.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be read.\n",
    "\n",
    "    Yields:\n",
    "        str: Each line of the file, stripped of leading and trailing whitespace.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), file_path)\n",
    "    buffer_size = 1024 * 1024\n",
    "    with open(full_path, 'r', buffering=buffer_size, encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            yield line.strip()\n",
    "\n",
    "\n",
    "def ls(dir_path: str, /) -> tuple[str, ...]:\n",
    "    \"\"\"\n",
    "    List all files and directories in the specified directory.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), dir_path)\n",
    "    return tuple(listdir(full_path))\n",
    "\n",
    "\n",
    "def mkdir(dir_path: str, /) -> None:\n",
    "    \"\"\"\n",
    "    Create a directory.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), dir_path)\n",
    "    if not path.exists(full_path):\n",
    "        makedirs(full_path)\n",
    "\n",
    "\n",
    "def path_exists(location: str, /) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the specified path exists.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), location)\n",
    "    return path.exists(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from threading import Thread, Lock\n",
    "from typing import final\n",
    "\n",
    "\n",
    "@final\n",
    "class BackgroundTask:\n",
    "    \"\"\"\n",
    "    Represents a background task that can be executed concurrently.\n",
    "\n",
    "    Args:\n",
    "        task (Callable): The function or method to be executed as a background task.\n",
    "        *args: Variable length argument list to be passed to the task.\n",
    "        **kwargs: Arbitrary keyword arguments to be passed to the task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task: Callable[..., None], *args, **kwargs):\n",
    "        self.__task = Thread(\n",
    "            target=task,\n",
    "            args=args,\n",
    "            kwargs=kwargs,\n",
    "            daemon=True\n",
    "        )\n",
    "        with Lock():\n",
    "            self.__task.start()\n",
    "\n",
    "    def wait(self) -> None:\n",
    "        \"\"\"\n",
    "        Waits for the background task to complete.\n",
    "        \"\"\"\n",
    "        return self.__task.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file_to(file_path: str, /, destination: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzip a file to a specified destination.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be unzipped.\n",
    "        destination (str): The path to the directory where the file will be unzipped.\n",
    "    \"\"\"\n",
    "    import zipfile\n",
    "    full_path = path.join(getcwd(), file_path)\n",
    "    with zipfile.ZipFile(full_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Final, Optional\n",
    "from shelve import open as shelve_open\n",
    "\n",
    "\n",
    "class GlobalCache:\n",
    "    \"\"\"\n",
    "    A simple global cache for storing data in memory.\n",
    "    \"\"\"\n",
    "\n",
    "    __runtime_cache: Final[dict[str, Any]] = {}\n",
    "    __cache_file_name: Final[str] = 'cache'\n",
    "\n",
    "    def put(self, key: str, value: object, /) -> None:\n",
    "        \"\"\"\n",
    "        Put a value into the cache.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key to be used to store the value.\n",
    "            value (object): The value to be stored.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__runtime_cache[key] = value\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            cache[key] = value\n",
    "\n",
    "    def get(self, key: str, /) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get a value from the cache.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key to be used to retrieve the value.\n",
    "\n",
    "        Returns:\n",
    "            object: The value stored in the cache.\n",
    "        \"\"\"\n",
    "\n",
    "        if key in self.__runtime_cache:\n",
    "            return self.__runtime_cache[key]\n",
    "\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            return cache.get(key)\n",
    "\n",
    "    def remove(self, key: str, /) -> None:\n",
    "        \"\"\"\n",
    "        Remove a value from the cache.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key to be used to remove the value.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__runtime_cache.pop(key, None)\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            del cache[key]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        Clear the cache.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__runtime_cache.clear()\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package imports for Application logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import regex as re\n",
    "import contractions\n",
    "import torch\n",
    "\n",
    "from os import cpu_count\n",
    "from typing import Final, final\n",
    "from types import NoneType\n",
    "from string import punctuation, digits\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from enum import Enum\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from collections.abc import Sequence\n",
    "from copy import copy\n",
    "from huggingface_hub import hf_hub_download\n",
    "from emoji import EMOJI_DATA, demojize\n",
    "from nltk.downloader import download as nltk_download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define global instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_path: Final[str] = 'data'\n",
    "glove_data_dir: Final[str] = f\"{dataset_base_path}/glove\"\n",
    "target_glove_file_name: Final[str] = \"glove.6B.100d.txt\"\n",
    "target_bert_model_name: Final[str] = 'bert-base-multilingual-uncased'\n",
    "target_mixtral_model_name: Final[str] = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "# names of the test set files\n",
    "test_set_names: Final[tuple[str, ...]] = (\n",
    "    'twitter-test1.txt',\n",
    "    'twitter-test2.txt',\n",
    "    'twitter-test3.txt',\n",
    ")\n",
    "training_data_file_name: Final[str] = 'twitter-training-data.txt'\n",
    "devlopment_data_file_name: Final[str] = 'twitter-dev-data.txt'\n",
    "\n",
    "\n",
    "@final\n",
    "class Sentiment(Enum):\n",
    "    \"\"\"\n",
    "    An enumeration of the three possible sentiment values.\n",
    "    \"\"\"\n",
    "    positive = 2\n",
    "    negative = 1\n",
    "    neutral = 0\n",
    "\n",
    "    @classmethod\n",
    "    @lru_cache\n",
    "    def gts(cls) -> tuple[str, ...]:\n",
    "        return tuple(cls.__members__.keys())\n",
    "\n",
    "\n",
    "global_cache = GlobalCache()\n",
    "\n",
    "TweetID = str\n",
    "ShouldMarkedAsBackground = NoneType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(typed=True)\n",
    "def get_tweets_from(file_name_: str, /) -> tuple[dict[TweetID, str], tuple[Sentiment, ...]]:\n",
    "    \"\"\"\n",
    "    Read tweets from a file and return dictionaries containing tweet IDs, contents, and sentiments.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name_ (str): The name of the file to read tweets from.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing two dictionaries:\n",
    "        - id_gts (dict[TweetID, str]): A dictionary mapping tweet IDs to their contents.\n",
    "        - id_sentiments (dict[TweetID, Sentiment]): A dictionary mapping tweet IDs to their sentiments.\n",
    "    \"\"\"\n",
    "    id_gts: OrderedDict[TweetID, str] = OrderedDict()\n",
    "    sentiments: list[Sentiment] = []\n",
    "    lines = read_file_lines_from(f'{dataset_base_path}/{file_name_}')\n",
    "    for line in lines:\n",
    "        fields = line.split('\\t')\n",
    "        tweet_id = fields[0]\n",
    "        gt = fields[1]\n",
    "        content = ' '.join(fields[2:])\n",
    "        id_gts[tweet_id] = content\n",
    "        sentiments.append(Sentiment[gt])\n",
    "\n",
    "    return id_gts, tuple(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define GloVe data preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_glove_data() -> ShouldMarkedAsBackground:\n",
    "    if path_exists(glove_data_dir) and len(ls(glove_data_dir)) == 4:\n",
    "        return\n",
    "\n",
    "    glove_data_pack_name = 'glove.6B.zip'\n",
    "\n",
    "    hf_hub_download(\n",
    "        repo_id='stanfordnlp/glove',\n",
    "        filename=glove_data_pack_name,\n",
    "        local_dir=dataset_base_path,\n",
    "        revision='1db2080b2d94def6e5b0386a523102f9d8849e9d',\n",
    "    )\n",
    "\n",
    "    # perform shell command using python code since the thread management can be done in python.\n",
    "    mkdir(glove_data_dir)\n",
    "    unzip_file_to(\n",
    "        f'{dataset_base_path}/{glove_data_pack_name}',\n",
    "        destination=glove_data_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(typed=True)\n",
    "def parse_glove_data(file_name_: str) -> tuple[dict[str, int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parse the GloVe data from a given file.\n",
    "\n",
    "    Args:\n",
    "        file_name_ (str): The name of the file containing the GloVe data.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[str, int], np.ndarray]: A tuple containing two elements:\n",
    "            - A dictionary mapping words to their corresponding indices.\n",
    "            - A numpy array containing the word vectors.\n",
    "    \"\"\"\n",
    "    file_frame = pd.read_csv(\n",
    "        f\"{glove_data_dir}/{file_name_}\",\n",
    "        delimiter=' ',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        encoding='utf-8',\n",
    "        skip_blank_lines=True,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        file_frame.reset_index().set_index(0)['index'].to_dict(),\n",
    "        file_frame.iloc[:, 1:].to_numpy(dtype=np.float64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(src: str, /, *, patterns: Sequence[re.Pattern]) -> str:\n",
    "    \"\"\"\n",
    "    Filters the given source text by removing all occurrences of the specified patterns.\n",
    "\n",
    "    Args:\n",
    "        src (str): The source text to be filtered.\n",
    "        patterns (Sequence[Pattern]): A sequence of regular expression patterns to be removed from the source text.\n",
    "\n",
    "    Returns:\n",
    "        str: The filtered text with all occurrences of the specified patterns removed.\n",
    "    \"\"\"\n",
    "    filtered = copy(src)\n",
    "\n",
    "    for pattern in patterns:\n",
    "        filtered = pattern.sub('', filtered)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def process_texts(src_dict: dict[Any, str], callable: Callable, *args, **kwargs) -> dict[Any, str]:\n",
    "    \"\"\"\n",
    "    Process a dictionary of texts using a callable function in parallel using a thread pool executor.\n",
    "\n",
    "    Args:\n",
    "        src_dict (dict[Any, str]): A dictionary containing the texts to be processed.\n",
    "        callable (Callable[[str], Any]): A callable function that will be applied to each text.\n",
    "        *args: Variable length argument list to be passed to the callable function.\n",
    "        **kargs: Arbitrary keyword arguments to be passed to the callable function.\n",
    "\n",
    "    Returns:\n",
    "        dict[Any, str]: A dictionary containing the processed texts.\n",
    "    \"\"\"\n",
    "\n",
    "    multi_process = False\n",
    "\n",
    "    if multi_process:\n",
    "        with ProcessPoolExecutor(max_workers=(cpu_count() or 1)) as executor:\n",
    "            future_to_key = {\n",
    "                executor.submit(callable, src_dict[key], *args, **kwargs): key for key in src_dict\n",
    "            }\n",
    "            return {\n",
    "                future_to_key[future]: future.result()\n",
    "                for future in as_completed(future_to_key)\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        key: callable(value, *args, **kwargs)\n",
    "        for key, value in src_dict.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def run_pipelines(\n",
    "    callables: Sequence[Callable[[str], str]],\n",
    "    /,\n",
    "    *,\n",
    "    tweets: dict[TweetID, str]\n",
    ") -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Run a sequence of callables on a dictionary of texts in parallel using a thread pool executor.\n",
    "\n",
    "    Args:\n",
    "        callables (Sequence[Callable[[dict[str, str]], dict[str, str]]]): A sequence of callable functions to be applied to the dictionary of texts.\n",
    "        tweets (dict[str, str]): A dictionary containing the texts to be processed.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, str]: A dictionary containing the processed texts.\n",
    "    \"\"\"\n",
    "    processed = copy(tweets)\n",
    "\n",
    "    for callable in callables:\n",
    "        processed = process_texts(processed, callable)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define confusion matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion(*, predict_results: dict[TweetID, Sentiment], test_set_file_name_: str) -> None:\n",
    "    \"\"\"\n",
    "    Display the confusion matrix based on the predicted results and the sentiment labels from the test set file.\n",
    "\n",
    "    Args:\n",
    "        predict_results (dict[TweetID, Sentiment]): A dictionary containing the predicted sentiment for each tweet ID.\n",
    "        test_set_file_name_ (str): The file name of the test set containing the sentiment labels for each tweet ID.\n",
    "    \"\"\"\n",
    "    id_tweets, sentiments = get_tweets_from(test_set_file_name_)\n",
    "\n",
    "    conf: Final[dict[Sentiment, dict[Sentiment, int]]] = defaultdict(\n",
    "        lambda: {\n",
    "            Sentiment.positive: 0,\n",
    "            Sentiment.negative: 0,\n",
    "            Sentiment.neutral: 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for tweet_id, sentiment in zip(id_tweets, sentiments):\n",
    "        if tweet_id in predict_results:\n",
    "            pred = predict_results[tweet_id]\n",
    "        else:\n",
    "            pred = Sentiment.neutral\n",
    "        conf[pred][sentiment] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(Sentiment.gts()))\n",
    "\n",
    "    for c1 in Sentiment:\n",
    "        print(c1.name.ljust(12), end='')\n",
    "        for c2 in Sentiment:\n",
    "            if c1_sum := sum(conf[c1].values()) > 0:\n",
    "                p = conf[c1][c2] / float(c1_sum)\n",
    "                print(f\"{p:.3f}     \", end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict_results: dict[TweetID, Sentiment], test_set_file_name_: str, classifier_name_: str) -> None:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a sentiment classifier by comparing the predicted results with the ground truth sentiment labels.\n",
    "\n",
    "    Parameters:\n",
    "        - predict_results: A dictionary mapping TweetIDs to predicted Sentiments.\n",
    "        - test_set_file_name_: The name of the test set file.\n",
    "        - classifier_name_: The name of the classifier.\n",
    "    \"\"\"\n",
    "    id_tweets, sentiments = get_tweets_from(test_set_file_name_)\n",
    "\n",
    "    acc_by_class: Final[dict[Sentiment, dict[str, int]]] = defaultdict(\n",
    "        lambda: {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "    )\n",
    "\n",
    "    for tweet_id, sentiment in zip(id_tweets, sentiments):\n",
    "        if tweet_id in predict_results:\n",
    "            pred = predict_results[tweet_id]\n",
    "        else:\n",
    "            pred = Sentiment.neutral\n",
    "\n",
    "        if sentiment == pred:\n",
    "            acc_by_class[sentiment]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[sentiment]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    cat_count = 0\n",
    "    item_count = 0\n",
    "    macro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "    micro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "    sem_eval_macro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "\n",
    "    micro_tp = 0.0\n",
    "    micro_fp = 0.0\n",
    "    micro_tn = 0.0\n",
    "    micro_fn = 0.0\n",
    "\n",
    "    cat_f1s: dict[Sentiment, float] = {}\n",
    "\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        cat_count += 1\n",
    "\n",
    "        micro_tp += acc['tp']\n",
    "        micro_fp += acc['fp']\n",
    "        micro_tn += acc['tn']\n",
    "        micro_fn += acc['fn']\n",
    "\n",
    "        p = 0.0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0.0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0.0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        cat_f1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            sem_eval_macro['p'] += p\n",
    "            sem_eval_macro['r'] += r\n",
    "            sem_eval_macro['f1'] += f1\n",
    "\n",
    "        item_count += n\n",
    "\n",
    "    micro['p'] = micro_tp / (micro_tp + micro_fp)\n",
    "    micro['r'] = micro_tp / (micro_tp + micro_fn)\n",
    "    micro['f1'] = 2 * micro['p'] * micro['r'] / (micro['p'] + micro['r'])\n",
    "\n",
    "    sem_eval_macro_f1 = sem_eval_macro['f1'] / 2\n",
    "\n",
    "    print(\n",
    "        f\"{test_set_file_name_} ({classifier_name_}): {sem_eval_macro_f1:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_sentiments = get_tweets_from(training_data_file_name)\n",
    "dev_data, dev_sentiments = get_tweets_from(devlopment_data_file_name)\n",
    "test_datas, test_sentiments = zip(*[\n",
    "    get_tweets_from(file_name)\n",
    "    for file_name in test_set_names\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download network resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_prepare_task = BackgroundTask(prepare_glove_data)\n",
    "\n",
    "\n",
    "def download_nltk_resources(resource_names: Sequence[str]) -> ShouldMarkedAsBackground:\n",
    "    for resource_name in resource_names:\n",
    "        nltk_download(resource_name, quiet=True)\n",
    "\n",
    "\n",
    "nltk_prepare_task = BackgroundTask(\n",
    "    download_nltk_resources,\n",
    "    ('stopwords', 'vader_lexicon', 'punkt', 'wordnet',)\n",
    ")\n",
    "\n",
    "\n",
    "def pre_load_bert_tokenizer() -> ShouldMarkedAsBackground:\n",
    "    BertTokenizer.from_pretrained(target_bert_model_name)\n",
    "    BertModel.from_pretrained(target_bert_model_name)\n",
    "\n",
    "\n",
    "bert_prepare_task = BackgroundTask(pre_load_bert_tokenizer)\n",
    "\n",
    "\n",
    "def pre_load_mixtral_model() -> ShouldMarkedAsBackground:\n",
    "    AutoTokenizer.from_pretrained(target_mixtral_model_name)\n",
    "    AutoModelForCausalLM.from_pretrained(target_mixtral_model_name)\n",
    "\n",
    "\n",
    "mixtral_prepare_task = BackgroundTask(pre_load_mixtral_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all the words.\n",
    "\n",
    "def lowercase_tweet(tweet: str, /) -> str:\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regexp filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the tweets based on the selected regexp patterns.\n",
    "\n",
    "re_flags = re.IGNORECASE | re.MULTILINE\n",
    "\n",
    "pattern_html_tags = re.compile(r'<[^>]+?>', re_flags)\n",
    "pattern_mentions = re.compile(r'@[a-zA-Z0-9_]+?', re_flags)\n",
    "pattern_hashtags = re.compile(r'#[a-zA-Z0-9_]+?', re_flags)\n",
    "pattern_alphanumeric = re.compile(r'[^a-zA-Z0-9 ]+?', re_flags)\n",
    "pattern_only_one_char = re.compile(r'\\b[a-zA-Z0-9]\\b', re_flags)\n",
    "pattern_fully_numeric = re.compile(r'\\b([0-9]+)\\b', re_flags)\n",
    "\n",
    "pattern_punctuation = re.compile(\n",
    "    \"[\" + re.escape(punctuation+\"“”…‘’\") + \"]+?\",\n",
    "    re_flags\n",
    ")\n",
    "\n",
    "pattern_url = re.compile(\n",
    "    r'(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?'\n",
    "    r'(//(?:[a-zA-Z0-9-._~%!$&\\'()*+,;=:]*(?::[a-zA-Z0-9-._~%!$&\\'()*+,;=:]+)?@)?'\n",
    "    r'(?:\\[[0-9a-fA-F:.]+]|(?:[a-zA-Z0-9-]+\\.)*[a-zA-Z]{2,}|[0-9.]+|localhost)'\n",
    "    r'(?::\\d+)?)(/[a-zA-Z0-9-._~%!$&\\'()*+,;=:@]*/?)*'\n",
    "    r'(?:\\?[a-zA-Z0-9-._~%!$&\\'()*+,;=:@/]*)?'\n",
    "    r'(?:#[a-zA-Z0-9-._~%!$&\\'()*+,;=:@/]*)?',\n",
    "    re_flags\n",
    ")\n",
    "\n",
    "all_emojis = tuple(EMOJI_DATA.keys())\n",
    "pattern_emojis = re.compile('|'.join(map(re.escape, all_emojis)) + '?')\n",
    "\n",
    "pattern_ampm = re.compile(r'([0-9]+(am|pm))')\n",
    "pattern_ordinals = re.compile(r'([0-9]+)(?:st|nd|rd|th)')\n",
    "\n",
    "selected_filter_patterns: tuple[re.Pattern[str], ...] = (\n",
    "    pattern_url,\n",
    "    pattern_html_tags,\n",
    "    pattern_mentions,\n",
    "    pattern_hashtags,\n",
    "    pattern_punctuation,\n",
    "    pattern_fully_numeric,\n",
    "    pattern_ampm,\n",
    "    pattern_ordinals,\n",
    ")\n",
    "\n",
    "\n",
    "def regexp_filter(tweet: str, /) -> str:\n",
    "    return filter_text(tweet, patterns=selected_filter_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(tweet: str, /) -> str:\n",
    "    return demojize(tweet, delimiters=('', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "\n",
    "\n",
    "def remove_numbers(tweet: str, /) -> str:\n",
    "    return tweet.translate(remove_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def remove_non_en(tweet: str, /) -> str:\n",
    "    try:\n",
    "        if detect(tweet) == 'en':\n",
    "            return tweet\n",
    "        else:\n",
    "            return ''\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_prepare_task.wait()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer(\n",
    "    reduce_len=True,\n",
    "    strip_handles=True,\n",
    "    preserve_case=False\n",
    ")\n",
    "stop_words = frozenset(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def nltk_tokenize_anti_stopwords_lemmatize(tweet: str, /) -> str:\n",
    "    wordnet.ensure_loaded()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    return ' '.join(\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_contractions(tweet: str, /) -> str:\n",
    "    return f'{contractions.fix(tweet)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run all preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pipelines = (\n",
    "    lowercase_tweet,\n",
    "    regexp_filter,\n",
    "    remove_emojis,\n",
    "    remove_numbers,\n",
    "    nltk_tokenize_anti_stopwords_lemmatize,\n",
    "    fix_contractions,\n",
    ")\n",
    "\n",
    "cleaned_training_tweets = run_pipelines(\n",
    "    all_pipelines,\n",
    "    tweets=training_data\n",
    ")\n",
    "\n",
    "cleaned_dev_tweets = run_pipelines(\n",
    "    all_pipelines,\n",
    "    tweets=dev_data\n",
    ")\n",
    "\n",
    "cleaned_test_tweets = [\n",
    "    run_pipelines(\n",
    "        all_pipelines,\n",
    "        tweets=test_data\n",
    "    )\n",
    "    for test_data in test_datas\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the top 10 most frequent words in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  freq\n",
      "0   tomorrow  5828\n",
      "1        may  5503\n",
      "2        not  3796\n",
      "3        day  3574\n",
      "4          i  3431\n",
      "5      going  3274\n",
      "6        you  3043\n",
      "7         am  2943\n",
      "8      night  2515\n",
      "9        see  2464\n",
      "10      like  2390\n",
      "11       get  2364\n",
      "12      time  2302\n",
      "13    sunday  2161\n",
      "14    friday  2075\n",
      "15       one  1937\n",
      "16        go  1860\n",
      "17      want  1846\n",
      "18       new  1692\n",
      "19      game  1631\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(cleaned_training_tweets.values()).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "freq_frame = pd.DataFrame(\n",
    "    word_freq.most_common(20),\n",
    "    columns=['word', 'freq']\n",
    ")\n",
    "\n",
    "print(freq_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orbes cupertino ca october apple ceo tim cook speaks event introducing new '\n",
      " 'iphone',\n",
      " 'true blood repeat sunday night',\n",
      " 'note woman blocked said hillary boring bernie would soaring may follow oops '\n",
      " 'blocked',\n",
      " 'bring back death penalty convicted paedophile answer simple vent',\n",
      " 'instead control gun use subsequent abuse criminally minded individualsthey '\n",
      " 'continue add',\n",
      " 'was not going murray tournament head cold tricky round opponent inspired '\n",
      " 'anderson performance',\n",
      " 'anyone want come disneyland afffrin tomorrow spot car',\n",
      " 'ice cube told you want get traightouttacompton theater august',\n",
      " 'jason year anniversary today baby make month today i am happiest luckiest '\n",
      " 'lady earth',\n",
      " 'kpop sound like disease get place sun definitely do not shine '\n",
      " 'irectionersfuneral',\n",
      " 'you are uncle texted get calibraska ep jack amp jack',\n",
      " 'david wright play tonight tomorrow st lucie wright sunday rejoin mets monday',\n",
      " 'bout see antman hope good ill report back tomorrow',\n",
      " 'long day work g meet marijuana march avenger sick nonetheless',\n",
      " 'idanispeaks could said marie colvin may liked employer never harmed anyone '\n",
      " 'family']\n",
      "14296\n"
     ]
    }
   ],
   "source": [
    "# TODO: Dev code.\n",
    "from pprint import pprint\n",
    "import random\n",
    "# generate a random number between 0 and 100\n",
    "\n",
    "window_size = 15\n",
    "rand_num = random.randint(0, len(cleaned_training_tweets) - window_size)\n",
    "\n",
    "pprint(list(cleaned_training_tweets.values())[rand_num:rand_num+window_size])\n",
    "print(rand_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45101, 2250)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    stop_words='english',\n",
    "    encoding='utf-8',\n",
    "    lowercase=False,\n",
    "    min_df=3,\n",
    "    max_df=0.8,\n",
    "    max_features=2250,\n",
    ")\n",
    "\n",
    "training_matrix = np.asarray(\n",
    "    csr_matrix(\n",
    "        vectorizer.fit_transform(cleaned_training_tweets.values())\n",
    "    ).todense()\n",
    ")\n",
    "dev_matrix = np.asarray(\n",
    "    csr_matrix(vectorizer.transform(cleaned_dev_tweets.values())).todense()\n",
    ")\n",
    "test_matrix = [\n",
    "    np.asarray(csr_matrix(vectorizer.transform(test_data.values())).todense())\n",
    "    for test_data in cleaned_test_tweets\n",
    "]\n",
    "\n",
    "print(training_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive    0.58202   0.83511   0.68596      1504\n",
      "    negative    0.66390   0.28725   0.40100       557\n",
      "     neutral    0.76148   0.58639   0.66257      1470\n",
      "\n",
      "    accuracy                        0.64514      3531\n",
      "   macro avg    0.66913   0.56958   0.58318      3531\n",
      "weighted avg    0.66965   0.64514   0.63127      3531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = LinearSVC(verbose=False, max_iter=1000, dual=False)\n",
    "model.fit(\n",
    "    training_matrix,\n",
    "    [sentiments.value for sentiments in training_sentiments]\n",
    ")\n",
    "svm_predictions = model.predict(test_matrix[0])\n",
    "\n",
    "svm_report = classification_report(\n",
    "    [sentiments.value for sentiments in test_sentiments[0]],\n",
    "    svm_predictions,\n",
    "    target_names=Sentiment.gts(),\n",
    "    digits=5\n",
    ")\n",
    "\n",
    "print(svm_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GloVe Embedding Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@final\n",
    "class GloVeVectorizer:\n",
    "    def __init__(self, word_indexes: dict[str, int], word_vectors: np.ndarray):\n",
    "        self.word_indexes = word_indexes\n",
    "        self.word_vectors = word_vectors\n",
    "\n",
    "    def __vectorize_impl(self, tokens: Sequence[str], /) -> np.ndarray:\n",
    "        vecs = np.zeros(\n",
    "            (len(tokens), self.word_vectors.shape[1]),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in self.word_indexes:\n",
    "                vecs[i] = self.word_vectors[self.word_indexes[token]]\n",
    "        return vecs\n",
    "\n",
    "    def __ordinalization_impl(self, tokens: Sequence[str], /) -> np.ndarray:\n",
    "        return np.asarray([\n",
    "            self.word_indexes.get(token, 400000) # 400000 is the index of the <UNK> token.\n",
    "            for token in tokens\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def __largest_element_count(element_sets: Sequence[Sequence | np.ndarray], /) -> int:\n",
    "        return max(\n",
    "            len(element)\n",
    "            for element in element_sets\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def __expand_2d_to_size(\n",
    "        _2d_targets: Sequence[np.ndarray],\n",
    "        /,\n",
    "        *,\n",
    "        size: int\n",
    "    ) -> list[np.ndarray]:\n",
    "        assert size > 0\n",
    "        assert len(_2d_targets) > 0\n",
    "        return [\n",
    "            np.vstack((\n",
    "                _2d_target,\n",
    "                np.zeros(\n",
    "                    (size - _2d_target.shape[0], _2d_target.shape[1]),\n",
    "                    dtype=np.float64\n",
    "                )\n",
    "            ))\n",
    "            for _2d_target in _2d_targets\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def __expand_1d_to_size(\n",
    "        _1d_targets: Sequence[np.ndarray],\n",
    "        /,\n",
    "        *,\n",
    "        size: int\n",
    "    ) -> list[np.ndarray]:\n",
    "        assert size > 0\n",
    "        assert len(_1d_targets) > 0\n",
    "        return [\n",
    "            np.pad(\n",
    "                _1d_target,\n",
    "                (0, size - _1d_target.shape[0]),\n",
    "                mode='constant',\n",
    "                constant_values=400000 # 400000 is the index of the <UNK> token.\n",
    "            )\n",
    "            for _1d_target in _1d_targets\n",
    "        ]\n",
    "\n",
    "    def vectorize(self, tweets: dict[TweetID, str], /) -> np.ndarray:\n",
    "        tokens = [\n",
    "            self.__vectorize_impl(tweet.split())\n",
    "            for tweet in tweets.values()\n",
    "        ]\n",
    "        return np.asarray(\n",
    "            self.__expand_2d_to_size(\n",
    "                tokens,\n",
    "                size=self.__largest_element_count(tokens)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def ordinalization(self, tweets: dict[TweetID, str], /) -> np.ndarray:\n",
    "        indexes = [\n",
    "            self.__ordinalization_impl(tweet.split())\n",
    "            for tweet in tweets.values()\n",
    "        ]\n",
    "        return np.asarray(\n",
    "            self.__expand_1d_to_size(\n",
    "                indexes,\n",
    "                size=self.__largest_element_count(indexes)\n",
    "            ),\n",
    "            dtype=np.int32\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechanism(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A module that implements an attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): The size of the hidden state.\n",
    "\n",
    "    Attributes:\n",
    "        hidden_size (int): The size of the hidden state.\n",
    "        attention_weights (torch.Tensor): The attention weights.\n",
    "\n",
    "    Methods:\n",
    "        forward(lstm_output: torch.Tensor) -> torch.Tensor: Performs the forward pass of the attention mechanism.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int) -> None:\n",
    "        super(AttentionMechanism, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_weights = torch.nn.Parameter(\n",
    "            torch.Tensor(hidden_size, 1)\n",
    "        )\n",
    "        torch.nn.init.xavier_uniform_(self.attention_weights.data, gain=1.414)\n",
    "\n",
    "    def forward(self, lstm_output: torch.Tensor) -> torch.Tensor:\n",
    "        attention_scores = torch.matmul(lstm_output, self.attention_weights)\n",
    "        attention_scores = attention_scores.squeeze(2)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        weighted_output = lstm_output * attention_weights.unsqueeze(2)\n",
    "        return weighted_output.sum(1)\n",
    "\n",
    "\n",
    "class SentimentLSTMModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A sentiment analysis LSTM model.\n",
    "\n",
    "    This model takes in a sequence of input tokens and predicts the sentiment of the input text.\n",
    "    It consists of an embedding layer, a bidirectional LSTM layer, an attention mechanism, and a fully connected layer.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        embedding_dim (int): The dimension of the word embeddings.\n",
    "        hidden_size (int): The size of the hidden state of the LSTM.\n",
    "        output_size (int): The number of output classes.\n",
    "        pretrained_embeddings (torch.Tensor): Pretrained word embeddings.\n",
    "\n",
    "    Attributes:\n",
    "        hidden_size (int): The size of the hidden state of the LSTM.\n",
    "        embedding (torch.nn.Embedding): The embedding layer.\n",
    "        lstm (torch.nn.LSTM): The bidirectional LSTM layer.\n",
    "        attention (AttentionMechanism): The attention mechanism.\n",
    "        fc (torch.nn.Linear): The fully connected layer.\n",
    "        dropout (torch.nn.Dropout): The dropout layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        pretrained_embeddings: torch.Tensor\n",
    "    ):\n",
    "        super(SentimentLSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(\n",
    "            embeddings=pretrained_embeddings,\n",
    "            freeze=False\n",
    "        )\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=0.5,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.attention = AttentionMechanism(hidden_size * 2)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        self.batch_norm = torch.nn.BatchNorm1d(output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, /) -> torch.Tensor:\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        attn_out = self.attention(lstm_out)\n",
    "        out = self.fc(self.dropout(attn_out))\n",
    "        return self.batch_norm(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define LSTM model training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    _, predicted_labels = torch.max(predictions, dim=1)\n",
    "    correct = (predicted_labels == labels).float()\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def process_epoch(\n",
    "    dataset_loader: DataLoader,\n",
    "    model: SentimentLSTMModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    is_training: bool = True\n",
    ") -> tuple[float, float]:\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, total_accuracy, batch_count = 0.0, 0.0, 0\n",
    "    for inputs, labels in dataset_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            predictions = model(inputs)\n",
    "            loss = criterion(predictions, labels)\n",
    "            if is_training:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += calculate_accuracy(predictions, labels)\n",
    "        batch_count += 1\n",
    "\n",
    "    avg_loss = total_loss / batch_count\n",
    "    avg_accuracy = total_accuracy / batch_count\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "\n",
    "def run_training_loop(\n",
    "    model: SentimentLSTMModel,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    n_epochs: int,\n",
    "    early_stopping_patience: int\n",
    ") -> None:\n",
    "    best_valid_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_accuracy = process_epoch(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            is_training=True\n",
    "        )\n",
    "        valid_loss, valid_accuracy = process_epoch(\n",
    "            valid_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            is_training=False\n",
    "        )\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_lstm_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}/{n_epochs} '\n",
    "            f'- Train Loss: {train_loss:.4f}, '\n",
    "            f'Train Accuracy: {train_accuracy:.4f}, '\n",
    "            f'Valid Loss: {valid_loss:.4f}, '\n",
    "            f'Valid Accuracy: {valid_accuracy:.4f}'\n",
    "        )\n",
    "\n",
    "\n",
    "def train_torch_model(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> float:\n",
    "    \"\"\"Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        train_loader: DataLoader for the training dataset.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "        device: The device to run the training on.\n",
    "\n",
    "    Returns:\n",
    "        The average loss for this training epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for tweets, sentiments in train_loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.set_grad_enabled(True), torch.cuda.amp.autocast():\n",
    "            outputs = model(tweets)\n",
    "            loss = loss_fn(outputs, sentiments)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "def evaluate_torch_model(\n",
    "    model: torch.nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Evaluate the model performance on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        val_loader: DataLoader for the validation dataset.\n",
    "        device: The device to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of accuracy and the average loss on the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for tweets, sentiments in val_loader:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            outputs = model(tweets)\n",
    "            loss = loss_fn(outputs, sentiments)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += sentiments.size(0)\n",
    "        correct += (predicted == sentiments).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    average_loss = total_loss / len(val_loader)\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get GloVe Embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_word_indexes: Optional[dict[str, int]] = global_cache.get('glove_word_indexes')\n",
    "glove_word_vectors: Optional[np.ndarray] = global_cache.get('glove_word_vectors')\n",
    "\n",
    "if (glove_word_indexes is None) or (glove_word_vectors is None):\n",
    "    glove_prepare_task.wait()\n",
    "    glove_word_indexes, glove_word_vectors = parse_glove_data(\n",
    "        target_glove_file_name\n",
    "    )\n",
    "    global_cache.put('glove_word_indexes', glove_word_indexes)\n",
    "    global_cache.put('glove_word_vectors', glove_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup the model, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTMModel(\n",
      "  (embedding): Embedding(400001, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (attention): AttentionMechanism()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (batch_norm): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "assert glove_word_indexes is not None\n",
    "assert glove_word_vectors is not None\n",
    "\n",
    "glove_vectorizer = GloVeVectorizer(glove_word_indexes, glove_word_vectors)\n",
    "\n",
    "training_glove_indexes = glove_vectorizer.ordinalization(\n",
    "    cleaned_training_tweets\n",
    ")\n",
    "\n",
    "dev_glove_indexes = glove_vectorizer.ordinalization(cleaned_dev_tweets)\n",
    "\n",
    "test_glove_indexs = [\n",
    "    glove_vectorizer.ordinalization(test_data)\n",
    "    for test_data in cleaned_test_tweets\n",
    "]\n",
    "\n",
    "training_dataset = TensorDataset(\n",
    "    torch.from_numpy(training_glove_indexes).to_dense().to(device),\n",
    "    torch.tensor(\n",
    "        [sentiments.value for sentiments in training_sentiments]\n",
    "    ).to_dense().to(device)\n",
    ")\n",
    "dev_dataset = TensorDataset(\n",
    "    torch.from_numpy(dev_glove_indexes).to_dense().to(device),\n",
    "    torch.tensor(\n",
    "        [sentiments.value for sentiments in dev_sentiments]\n",
    "    ).to_dense().to(device)\n",
    ")\n",
    "test_datasets = [\n",
    "    TensorDataset(\n",
    "        torch.from_numpy(test_glove_index).to_dense().to(device),\n",
    "        torch.tensor(\n",
    "            [sentiments.value for sentiments in test_sentiments]\n",
    "        ).to_dense().to(device)\n",
    "    )\n",
    "    for test_glove_index, test_sentiments in zip(\n",
    "        test_glove_indexs,\n",
    "        test_sentiments\n",
    "    )\n",
    "]\n",
    "\n",
    "lstm_model = SentimentLSTMModel(\n",
    "    embedding_dim=glove_word_vectors.shape[1],\n",
    "    hidden_size=256,\n",
    "    output_size=len(Sentiment.gts()),\n",
    "    pretrained_embeddings=torch.from_numpy(glove_word_vectors).float()\n",
    ").to(device)\n",
    "\n",
    "lstm_loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "lstm_optimizer = torch.optim.AdamW(\n",
    "    lstm_model.parameters(),\n",
    "    lr=1e-2,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.94278, Dev Loss: 0.86553, Dev Accuracy: 0.56700\n",
      "Epoch 2/10, Train Loss: 0.85556, Dev Loss: 0.81170, Dev Accuracy: 0.60500\n",
      "Epoch 3/10, Train Loss: 0.83088, Dev Loss: 0.77129, Dev Accuracy: 0.64900\n",
      "Epoch 4/10, Train Loss: 0.81411, Dev Loss: 0.77182, Dev Accuracy: 0.63850\n",
      "Epoch 5/10, Train Loss: 0.80169, Dev Loss: 0.78092, Dev Accuracy: 0.63850\n",
      "Epoch 6/10, Train Loss: 0.79249, Dev Loss: 0.74451, Dev Accuracy: 0.66100\n",
      "Epoch 7/10, Train Loss: 0.78254, Dev Loss: 0.77573, Dev Accuracy: 0.64500\n",
      "Epoch 8/10, Train Loss: 0.77808, Dev Loss: 0.73531, Dev Accuracy: 0.66250\n",
      "Epoch 9/10, Train Loss: 0.77335, Dev Loss: 0.75401, Dev Accuracy: 0.65200\n",
      "Epoch 10/10, Train Loss: 0.77335, Dev Loss: 0.75981, Dev Accuracy: 0.65450\n",
      "Test 1 Accuracy: 0.65704\n",
      "Test 2 Accuracy: 0.66271\n",
      "Test 3 Accuracy: 0.64649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10 (pre_load_mixtral_model):\n",
      "Traceback (most recent call last):\n",
      "  File \"/dcs/pg23/u5556162/.pyenv/versions/3.12.2/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/dcs/pg23/u5556162/.cache/pypoetry/virtualenvs/cs918-lab-nDh98LVA-py3.12/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/dcs/pg23/u5556162/.pyenv/versions/3.12.2/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/dcs-tmp.u5556162/ipykernel_684045/243090160.py\", line 25, in pre_load_mixtral_model\n",
      "  File \"/dcs/pg23/u5556162/.cache/pypoetry/virtualenvs/cs918-lab-nDh98LVA-py3.12/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/dcs/pg23/u5556162/.cache/pypoetry/virtualenvs/cs918-lab-nDh98LVA-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3264, in from_pretrained\n",
      "    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n",
      "                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/dcs/pg23/u5556162/.cache/pypoetry/virtualenvs/cs918-lab-nDh98LVA-py3.12/lib/python3.12/site-packages/transformers/utils/hub.py\", line 1038, in get_checkpoint_shard_files\n",
      "    cached_filename = cached_file(\n",
      "                      ^^^^^^^^^^^^\n",
      "  File \"/dcs/pg23/u5556162/.cache/pypoetry/virtualenvs/cs918-lab-nDh98LVA-py3.12/lib/python3.12/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"/dcs/pg23/u5556162/.cache/pypoetry/virtualenvs/cs918-lab-nDh98LVA-py3.12/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/dcs/pg23/u5556162/.cache/pypoetry/virtualenvs/cs918-lab-nDh98LVA-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1492, in hf_hub_download\n",
      "    http_get(\n",
      "  File \"/dcs/pg23/u5556162/.cache/pypoetry/virtualenvs/cs918-lab-nDh98LVA-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 550, in http_get\n",
      "    temp_file.write(chunk)\n",
      "  File \"/dcs/pg23/u5556162/.pyenv/versions/3.12.2/lib/python3.12/tempfile.py\", line 499, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 122] Disk quota exceeded\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 512\n",
    "workers = cpu_count() or 1\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_loaders = [\n",
    "    DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    for test_dataset in test_datasets\n",
    "]\n",
    "torch.jit.enable_onednn_fusion(True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_torch_model(\n",
    "        lstm_model,\n",
    "        training_loader,\n",
    "        lstm_loss_fn,\n",
    "        lstm_optimizer\n",
    "    )\n",
    "    dev_accuracy, dev_loss = evaluate_torch_model(\n",
    "        lstm_model,\n",
    "        dev_loader,\n",
    "        lstm_loss_fn\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.5f}, \"\n",
    "        f\"Dev Loss: {dev_loss:.5f}, Dev Accuracy: {dev_accuracy:.5f}\"\n",
    "    )\n",
    "\n",
    "test_accuracies = [\n",
    "    evaluate_torch_model(\n",
    "        lstm_model,\n",
    "        test_loader,\n",
    "        lstm_loss_fn\n",
    "    )\n",
    "    for test_loader in test_loaders\n",
    "]\n",
    "\n",
    "for i, (test_accuracy, _) in enumerate(test_accuracies):\n",
    "    print(f\"Test {i+1} Accuracy: {test_accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bert_prepare_task.wait()\n",
    "mixtral_prepare_task.wait()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(target_bert_model_name)\n",
    "model = BertModel.from_pretrained(target_bert_model_name)\n",
    "\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
