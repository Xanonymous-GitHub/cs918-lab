{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a Jupyter notebook for the coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, don’t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from os import path, getcwd, listdir, makedirs\n",
    "\n",
    "\n",
    "def read_file_lines_from(file_path: str, /) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Read lines from a file and yield each line as a string.\n",
    "    The path to the file is relative to the current working directory.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be read.\n",
    "\n",
    "    Yields:\n",
    "        str: Each line of the file, stripped of leading and trailing whitespace.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), file_path)\n",
    "    buffer_size = 1024 * 1024\n",
    "    with open(full_path, 'r', buffering=buffer_size, encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            yield line.strip()\n",
    "\n",
    "\n",
    "def ls(dir_path: str, /) -> tuple[str, ...]:\n",
    "    \"\"\"\n",
    "    List all files and directories in the specified directory.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), dir_path)\n",
    "    return tuple(listdir(full_path))\n",
    "\n",
    "\n",
    "def mkdir(dir_path: str, /) -> None:\n",
    "    \"\"\"\n",
    "    Create a directory.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), dir_path)\n",
    "    if not path.exists(full_path):\n",
    "        makedirs(full_path)\n",
    "\n",
    "\n",
    "def path_exists(location: str, /) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the specified path exists.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), location)\n",
    "    return path.exists(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from threading import Thread, Lock\n",
    "from typing import final\n",
    "\n",
    "\n",
    "@final\n",
    "class BackgroundTask:\n",
    "    \"\"\"\n",
    "    Represents a background task that can be executed concurrently.\n",
    "\n",
    "    Args:\n",
    "        task (Callable): The function or method to be executed as a background task.\n",
    "        *args: Variable length argument list to be passed to the task.\n",
    "        **kwargs: Arbitrary keyword arguments to be passed to the task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task: Callable[..., None], *args, **kwargs):\n",
    "        self.__task = Thread(\n",
    "            target=task,\n",
    "            args=args,\n",
    "            kwargs=kwargs,\n",
    "            daemon=True\n",
    "        )\n",
    "        with Lock():\n",
    "            self.__task.start()\n",
    "\n",
    "    def wait(self) -> None:\n",
    "        \"\"\"\n",
    "        Waits for the background task to complete.\n",
    "        \"\"\"\n",
    "        return self.__task.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file_to(file_path: str, /, destination: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzip a file to a specified destination.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be unzipped.\n",
    "        destination (str): The path to the directory where the file will be unzipped.\n",
    "    \"\"\"\n",
    "    import zipfile\n",
    "    full_path = path.join(getcwd(), file_path)\n",
    "    with zipfile.ZipFile(full_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Final, Optional\n",
    "from shelve import open as shelve_open\n",
    "\n",
    "\n",
    "class GlobalCache:\n",
    "    \"\"\"\n",
    "    A simple global cache for storing data in memory.\n",
    "    \"\"\"\n",
    "\n",
    "    __runtime_cache: Final[dict[str, Any]] = {}\n",
    "    __cache_file_name: Final[str] = 'cache'\n",
    "\n",
    "    def put(self, key: str, value: object, /) -> None:\n",
    "        \"\"\"\n",
    "        Put a value into the cache.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key to be used to store the value.\n",
    "            value (object): The value to be stored.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__runtime_cache[key] = value\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            cache[key] = value\n",
    "\n",
    "    def get(self, key: str, /) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get a value from the cache.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key to be used to retrieve the value.\n",
    "\n",
    "        Returns:\n",
    "            object: The value stored in the cache.\n",
    "        \"\"\"\n",
    "\n",
    "        if key in self.__runtime_cache:\n",
    "            return self.__runtime_cache[key]\n",
    "\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            return cache.get(key)\n",
    "\n",
    "    def remove(self, key: str, /) -> None:\n",
    "        \"\"\"\n",
    "        Remove a value from the cache.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key to be used to remove the value.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__runtime_cache.pop(key, None)\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            del cache[key]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        Clear the cache.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__runtime_cache.clear()\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package imports for Application logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import regex as re\n",
    "import contractions\n",
    "\n",
    "from os import cpu_count\n",
    "from typing import Final, final\n",
    "from types import NoneType\n",
    "from string import punctuation\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor, Future\n",
    "from enum import Enum\n",
    "from collections import defaultdict, Counter\n",
    "from collections.abc import Sequence\n",
    "from copy import copy\n",
    "from huggingface_hub import hf_hub_download\n",
    "from emoji import EMOJI_DATA, demojize\n",
    "from nltk.downloader import download as nltk_download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define global instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_path: Final[str] = 'data'\n",
    "glove_data_dir: Final[str] = f\"{dataset_base_path}/glove\"\n",
    "target_glove_file_name: Final[str] = \"glove.6B.100d.txt\"\n",
    "\n",
    "# names of the test set files\n",
    "test_set_names: Final[tuple[str, ...]] = (\n",
    "    'twitter-test1.txt',\n",
    "    'twitter-test2.txt',\n",
    "    'twitter-test3.txt',\n",
    ")\n",
    "training_data_file_name: Final[str] = 'twitter-training-data.txt'\n",
    "devlopment_data_file_name: Final[str] = 'twitter-dev-data.txt'\n",
    "\n",
    "\n",
    "@final\n",
    "class Sentiment(Enum):\n",
    "    \"\"\"\n",
    "    An enumeration of the three possible sentiment values.\n",
    "    \"\"\"\n",
    "    positive = 1\n",
    "    negative = -1\n",
    "    neutral = 0\n",
    "\n",
    "    @classmethod\n",
    "    @lru_cache\n",
    "    def gts(cls) -> tuple[str, ...]:\n",
    "        return tuple(cls.__members__.keys())\n",
    "\n",
    "\n",
    "global_cache = GlobalCache()\n",
    "\n",
    "TweetID = str\n",
    "ShouldMarkedAsBackground = NoneType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(typed=True)\n",
    "def get_tweets_from(file_name_: str, /) -> tuple[dict[TweetID, str], dict[TweetID, Sentiment]]:\n",
    "    \"\"\"\n",
    "    Read tweets from a file and return dictionaries containing tweet IDs, contents, and sentiments.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name_ (str): The name of the file to read tweets from.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing two dictionaries:\n",
    "        - id_gts (dict[TweetID, str]): A dictionary mapping tweet IDs to their contents.\n",
    "        - id_sentiments (dict[TweetID, Sentiment]): A dictionary mapping tweet IDs to their sentiments.\n",
    "    \"\"\"\n",
    "    id_gts: dict[TweetID, str] = {}\n",
    "    id_sentiments: dict[TweetID, Sentiment] = {}\n",
    "    lines = read_file_lines_from(f'{dataset_base_path}/{file_name_}')\n",
    "    for line in lines:\n",
    "        fields = line.split('\\t')\n",
    "        tweet_id = fields[0]\n",
    "        gt = fields[1]\n",
    "        content = ' '.join(fields[2:])\n",
    "        id_gts[tweet_id] = content\n",
    "        id_sentiments[tweet_id] = Sentiment[gt]\n",
    "\n",
    "    return id_gts, id_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define GloVe data preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_glove_data() -> ShouldMarkedAsBackground:\n",
    "    if path_exists(glove_data_dir) and len(ls(glove_data_dir)) == 4:\n",
    "        return\n",
    "\n",
    "    glove_data_pack_name = 'glove.6B.zip'\n",
    "\n",
    "    hf_hub_download(\n",
    "        repo_id='stanfordnlp/glove',\n",
    "        filename=glove_data_pack_name,\n",
    "        local_dir=dataset_base_path,\n",
    "        revision='1db2080b2d94def6e5b0386a523102f9d8849e9d',\n",
    "    )\n",
    "\n",
    "    # perform shell command using python code since the thread management can be done in python.\n",
    "    mkdir(glove_data_dir)\n",
    "    unzip_file_to(\n",
    "        f'{dataset_base_path}/{glove_data_pack_name}',\n",
    "        destination=glove_data_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(typed=True)\n",
    "def parse_glove_data(file_name_: str) -> tuple[dict[int, str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parse the GloVe data from a given file.\n",
    "\n",
    "    Args:\n",
    "        file_name_ (str): The name of the file containing the GloVe data.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, str], np.ndarray]: A tuple containing a dictionary mapping integers to words and a NumPy array of word vectors.\n",
    "    \"\"\"\n",
    "    file_frame = pd.read_csv(\n",
    "        f\"{glove_data_dir}/{file_name_}\",\n",
    "        delimiter=' ',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        encoding='utf-8',\n",
    "        skip_blank_lines=True,\n",
    "    )\n",
    "\n",
    "    return file_frame[0].to_dict(), file_frame.iloc[:, 1:].to_numpy(dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(src: str, /, *, patterns: Sequence[re.Pattern]) -> str:\n",
    "    \"\"\"\n",
    "    Filters the given source text by removing all occurrences of the specified patterns.\n",
    "\n",
    "    Args:\n",
    "        src (str): The source text to be filtered.\n",
    "        patterns (Sequence[Pattern]): A sequence of regular expression patterns to be removed from the source text.\n",
    "\n",
    "    Returns:\n",
    "        str: The filtered text with all occurrences of the specified patterns removed.\n",
    "    \"\"\"\n",
    "    filtered = copy(src)\n",
    "\n",
    "    for pattern in patterns:\n",
    "        filtered = pattern.sub('', filtered)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def process_texts(src_dict: dict[Any, str], callable: Callable, *args, **kargs) -> dict[Any, str]:\n",
    "    \"\"\"\n",
    "    Process a dictionary of texts using a callable function in parallel using a thread pool executor.\n",
    "\n",
    "    Args:\n",
    "        src_dict (dict[Any, str]): A dictionary containing the texts to be processed.\n",
    "        callable (Callable[[str], Any]): A callable function that will be applied to each text.\n",
    "        *args: Variable length argument list to be passed to the callable function.\n",
    "        **kargs: Arbitrary keyword arguments to be passed to the callable function.\n",
    "\n",
    "    Returns:\n",
    "        dict[Any, str]: A dictionary containing the processed texts.\n",
    "    \"\"\"\n",
    "\n",
    "    multi_threaded = False\n",
    "\n",
    "    result: dict[Any, Future[str]] = {}\n",
    "\n",
    "    if multi_threaded:\n",
    "        with ThreadPoolExecutor(max_workers=(cpu_count() or 1)+4) as executor:\n",
    "            for key, value in src_dict.items():\n",
    "                result[key] = executor.submit(\n",
    "                    callable,\n",
    "                    value,\n",
    "                    *args,\n",
    "                    **kargs,\n",
    "                )\n",
    "\n",
    "        return {key: future.result() for key, future in result.items()}\n",
    "\n",
    "    return {\n",
    "        key: callable(value, *args, **kargs)\n",
    "        for key, value in src_dict.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def run_pipelines(\n",
    "    callables: Sequence[Callable[[str], str]],\n",
    "    /,\n",
    "    *,\n",
    "    tweets: dict[TweetID, str]\n",
    ") -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Run a sequence of callables on a dictionary of texts in parallel using a thread pool executor.\n",
    "\n",
    "    Args:\n",
    "        callables (Sequence[Callable[[dict[str, str]], dict[str, str]]]): A sequence of callable functions to be applied to the dictionary of texts.\n",
    "        tweets (dict[str, str]): A dictionary containing the texts to be processed.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, str]: A dictionary containing the processed texts.\n",
    "    \"\"\"\n",
    "    processed = copy(tweets)\n",
    "\n",
    "    for callable in callables:\n",
    "        processed = process_texts(processed, callable)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define confusion matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion(*, predict_results: dict[TweetID, Sentiment], test_set_file_name_: str) -> None:\n",
    "    \"\"\"\n",
    "    Display the confusion matrix based on the predicted results and the sentiment labels from the test set file.\n",
    "\n",
    "    Args:\n",
    "        predict_results (dict[TweetID, Sentiment]): A dictionary containing the predicted sentiment for each tweet ID.\n",
    "        test_set_file_name_ (str): The file name of the test set containing the sentiment labels for each tweet ID.\n",
    "    \"\"\"\n",
    "    _, id_sentiments = get_tweets_from(test_set_file_name_)\n",
    "\n",
    "    conf: Final[dict[Sentiment, dict[Sentiment, int]]] = defaultdict(\n",
    "        lambda: {\n",
    "            Sentiment.positive: 0,\n",
    "            Sentiment.negative: 0,\n",
    "            Sentiment.neutral: 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for tweet_id, sentiment in id_sentiments.items():\n",
    "        if tweet_id in predict_results:\n",
    "            pred = predict_results[tweet_id]\n",
    "        else:\n",
    "            pred = Sentiment.neutral\n",
    "        conf[pred][sentiment] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(Sentiment.gts()))\n",
    "\n",
    "    for c1 in Sentiment:\n",
    "        print(c1.name.ljust(12), end='')\n",
    "        for c2 in Sentiment:\n",
    "            if c1_sum := sum(conf[c1].values()) > 0:\n",
    "                p = conf[c1][c2] / float(c1_sum)\n",
    "                print(f\"{p:.3f}     \", end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict_results: dict[TweetID, Sentiment], test_set_file_name_: str, classifier_name_: str) -> None:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a sentiment classifier by comparing the predicted results with the ground truth sentiment labels.\n",
    "\n",
    "    Parameters:\n",
    "        - predict_results: A dictionary mapping TweetIDs to predicted Sentiments.\n",
    "        - test_set_file_name_: The name of the test set file.\n",
    "        - classifier_name_: The name of the classifier.\n",
    "    \"\"\"\n",
    "    _, id_sentiments = get_tweets_from(test_set_file_name_)\n",
    "\n",
    "    acc_by_class: Final[dict[Sentiment, dict[str, int]]] = defaultdict(\n",
    "        lambda: {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "    )\n",
    "\n",
    "    for tweet_id, sentiment in id_sentiments.items():\n",
    "        if tweet_id in predict_results:\n",
    "            pred = predict_results[tweet_id]\n",
    "        else:\n",
    "            pred = Sentiment.neutral\n",
    "\n",
    "        if sentiment == pred:\n",
    "            acc_by_class[sentiment]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[sentiment]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    cat_count = 0\n",
    "    item_count = 0\n",
    "    macro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "    micro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "    sem_eval_macro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "\n",
    "    micro_tp = 0.0\n",
    "    micro_fp = 0.0\n",
    "    micro_tn = 0.0\n",
    "    micro_fn = 0.0\n",
    "\n",
    "    cat_f1s: dict[Sentiment, float] = {}\n",
    "\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        cat_count += 1\n",
    "\n",
    "        micro_tp += acc['tp']\n",
    "        micro_fp += acc['fp']\n",
    "        micro_tn += acc['tn']\n",
    "        micro_fn += acc['fn']\n",
    "\n",
    "        p = 0.0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0.0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0.0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        cat_f1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            sem_eval_macro['p'] += p\n",
    "            sem_eval_macro['r'] += r\n",
    "            sem_eval_macro['f1'] += f1\n",
    "\n",
    "        item_count += n\n",
    "\n",
    "    micro['p'] = micro_tp / (micro_tp + micro_fp)\n",
    "    micro['r'] = micro_tp / (micro_tp + micro_fn)\n",
    "    micro['f1'] = 2 * micro['p'] * micro['r'] / (micro['p'] + micro['r'])\n",
    "\n",
    "    sem_eval_macro_f1 = sem_eval_macro['f1'] / 2\n",
    "\n",
    "    print(\n",
    "        f\"{test_set_file_name_} ({classifier_name_}): {sem_eval_macro_f1:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_sentiments = get_tweets_from(training_data_file_name)\n",
    "dev_data, dev_sentiments = get_tweets_from(devlopment_data_file_name)\n",
    "test_datas, test_sentiments = zip(*[\n",
    "    get_tweets_from(file_name)\n",
    "    for file_name in test_set_names\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download network resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_prepare_task = BackgroundTask(prepare_glove_data)\n",
    "\n",
    "\n",
    "def download_nltk_resources(resource_names: Sequence[str]) -> ShouldMarkedAsBackground:\n",
    "    for resource_name in resource_names:\n",
    "        nltk_download(resource_name, quiet=True)\n",
    "\n",
    "\n",
    "nltk_prepare_task = BackgroundTask(\n",
    "    download_nltk_resources,\n",
    "    ('stopwords', 'vader_lexicon', 'punkt', 'wordnet',)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this cell to the top of the place where glove is used\n",
    "\n",
    "glove_word_indexes = global_cache.get('glove_word_indexes')\n",
    "glove_word_vectors = global_cache.get('glove_word_vectors')\n",
    "\n",
    "if glove_word_indexes is None or glove_word_vectors is None:\n",
    "    glove_prepare_task.wait()\n",
    "    glove_word_indexes, glove_word_vectors = parse_glove_data(\n",
    "        target_glove_file_name\n",
    "    )\n",
    "    global_cache.put('glove_word_indexes', glove_word_indexes)\n",
    "    global_cache.put('glove_word_vectors', glove_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all the words.\n",
    "\n",
    "def lowercase_tweet(tweet: str, /) -> str:\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the tweets based on the selected regexp patterns.\n",
    "\n",
    "re_flags = re.IGNORECASE | re.MULTILINE\n",
    "\n",
    "pattern_html_tags = re.compile(r'<[^>]+?>', re_flags)\n",
    "pattern_mentions = re.compile(r'@[a-zA-Z0-9_]+', re_flags)\n",
    "pattern_hashtags = re.compile(r'#[a-zA-Z0-9_]+', re_flags)\n",
    "pattern_alphanumeric = re.compile(r'[^a-zA-Z0-9 ]+?', re_flags)\n",
    "pattern_only_one_char = re.compile(r'\\b[a-zA-Z0-9]\\b', re_flags)\n",
    "pattern_fully_numeric = re.compile(r'\\b\\d+?\\b', re_flags)\n",
    "\n",
    "pattern_punctuation = re.compile(\n",
    "    \"[\" + re.escape(punctuation+\"“”…‘’\") + \"]+?\",\n",
    "    re_flags\n",
    ")\n",
    "\n",
    "pattern_url = re.compile(\n",
    "    r'(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?'\n",
    "    r'(//(?:[a-zA-Z0-9-._~%!$&\\'()*+,;=:]*(?::[a-zA-Z0-9-._~%!$&\\'()*+,;=:]+)?@)?'\n",
    "    r'(?:\\[[0-9a-fA-F:.]+]|(?:[a-zA-Z0-9-]+\\.)*[a-zA-Z]{2,}|[0-9.]+|localhost)'\n",
    "    r'(?::\\d+)?)(/[a-zA-Z0-9-._~%!$&\\'()*+,;=:@]*/?)*'\n",
    "    r'(?:\\?[a-zA-Z0-9-._~%!$&\\'()*+,;=:@/]*)?'\n",
    "    r'(?:#[a-zA-Z0-9-._~%!$&\\'()*+,;=:@/]*)?',\n",
    "    re_flags\n",
    ")\n",
    "\n",
    "all_emojis = tuple(EMOJI_DATA.keys())\n",
    "pattern_emojis = re.compile('|'.join(map(re.escape, all_emojis)) + '?')\n",
    "\n",
    "pattern_ampm = re.compile(r'([0-9]+(am|pm))')\n",
    "\n",
    "selected_filter_patterns: tuple[re.Pattern[str], ...] = (\n",
    "    pattern_url,\n",
    "    pattern_html_tags,\n",
    "    pattern_mentions,\n",
    "    pattern_hashtags,\n",
    "    pattern_punctuation,\n",
    "    pattern_fully_numeric,\n",
    "    pattern_ampm,\n",
    ")\n",
    "\n",
    "\n",
    "def regexp_filter(tweet: str, /) -> str:\n",
    "    return filter_text(tweet, patterns=selected_filter_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(tweet: str, /) -> str:\n",
    "    return demojize(tweet, delimiters=('', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def remove_non_en(tweet: str, /) -> str:\n",
    "    try:\n",
    "        if detect(tweet) == 'en':\n",
    "            return tweet\n",
    "        else:\n",
    "            return ''\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_prepare_task.wait()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer(\n",
    "    reduce_len=True,\n",
    "    strip_handles=True,\n",
    "    preserve_case=False\n",
    ")\n",
    "stop_words = frozenset(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def nltk_tokenize_anti_stopwords_lemmatize(tweet: str, /) -> str:\n",
    "    wordnet.ensure_loaded()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    return ' '.join(\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_contractions(tweet: str, /) -> str:\n",
    "    return f'{contractions.fix(tweet)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run all preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pipelines = (\n",
    "    lowercase_tweet,\n",
    "    regexp_filter,\n",
    "    remove_emojis,\n",
    "    # remove_non_en, TODO: optimize for performance\n",
    "    nltk_tokenize_anti_stopwords_lemmatize,\n",
    "    fix_contractions,\n",
    ")\n",
    "\n",
    "cleaned_training_tweets = run_pipelines(\n",
    "    all_pipelines,\n",
    "    tweets=training_data\n",
    ")\n",
    "\n",
    "cleaned_dev_tweets = run_pipelines(\n",
    "    all_pipelines,\n",
    "    tweets=dev_data\n",
    ")\n",
    "\n",
    "cleaned_test_tweets = [\n",
    "    run_pipelines(\n",
    "        all_pipelines,\n",
    "        tweets=test_data\n",
    "    )\n",
    "    for test_data in test_datas\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the top 10 most frequent words in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  freq\n",
      "0   tomorrow  5829\n",
      "1        may  5503\n",
      "2        not  3796\n",
      "3        day  3538\n",
      "4          i  3417\n",
      "5      going  3274\n",
      "6         am  2937\n",
      "7        you  2848\n",
      "8      night  2511\n",
      "9        see  2466\n",
      "10      like  2392\n",
      "11       get  2361\n",
      "12      time  2297\n",
      "13    sunday  2162\n",
      "14       1st  2158\n",
      "15    friday  2077\n",
      "16       one  1933\n",
      "17        go  1860\n",
      "18      want  1847\n",
      "19       new  1692\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(cleaned_training_tweets.values()).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "freq_frame = pd.DataFrame(\n",
    "    word_freq.most_common(20),\n",
    "    columns=['word', 'freq']\n",
    ")\n",
    "\n",
    "print(freq_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['post seinfeld tv today first thing tomorrow',\n",
      " 'ru showing black v tonga game friday nite sound definitely showing league '\n",
      " 'right',\n",
      " 'bold prediction think tim howard going to get hattie tomorrow long range '\n",
      " 'effort goal',\n",
      " 'bomb squad bake sale thursday 25th do not miss',\n",
      " 'july 23rd national hot dog day come celebrate you blake brock deck',\n",
      " 'happy birthday gorgeous may day magical movie magic mike xxl p',\n",
      " 'look eye vine',\n",
      " 'remember iron maiden album drop friday',\n",
      " 'gucci inexpressibles ego acceptation may embody twosome meaningful open door '\n",
      " 'junta elenchus extan',\n",
      " 'omg happy 2nd birthday baby i am obsessed little monkey prince george stay '\n",
      " 'adorbs cutie',\n",
      " 'pointless committee one need right kim jongun',\n",
      " 'lol keller insinuating ric flair may fed meltzer cena holding charlotte '\n",
      " 'story',\n",
      " 'become 1st footballer sportsowned legendary rapper jayz',\n",
      " 'good day vids tomorrow stasis long dark rebirth minecraft',\n",
      " 'fake ua activist buy fake follower cheek call faker']\n",
      "13458\n"
     ]
    }
   ],
   "source": [
    "# TODO: Dev code.\n",
    "from pprint import pprint\n",
    "import random\n",
    "# generate a random number between 0 and 100\n",
    "\n",
    "window_size = 15\n",
    "rand_num = random.randint(0, len(cleaned_training_tweets) - window_size)\n",
    "\n",
    "pprint(list(cleaned_training_tweets.values())[rand_num:rand_num+window_size])\n",
    "print(rand_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45101, 11313)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    stop_words='english',\n",
    "    encoding='utf-8',\n",
    "    lowercase=False,\n",
    "    min_df=3,\n",
    "    max_df=0.8,\n",
    ")\n",
    "\n",
    "training_matrix = np.asarray(\n",
    "    csr_matrix(\n",
    "        vectorizer.fit_transform(cleaned_training_tweets.values())\n",
    "    ).todense()\n",
    ")\n",
    "dev_matrix = np.asarray(\n",
    "    csr_matrix(vectorizer.transform(cleaned_dev_tweets.values())).todense()\n",
    ")\n",
    "test_matrix = [\n",
    "    np.asarray(csr_matrix(vectorizer.transform(test_data.values())).todense())\n",
    "    for test_data in cleaned_test_tweets\n",
    "]\n",
    "\n",
    "print(training_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive    0.57460   0.32496   0.41514       557\n",
      "    negative    0.57030   0.72008   0.63650      1504\n",
      "     neutral    0.65831   0.58980   0.62217      1470\n",
      "\n",
      "    accuracy                        0.60351      3531\n",
      "   macro avg    0.60107   0.54494   0.55794      3531\n",
      "weighted avg    0.60762   0.60351   0.59562      3531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = LinearSVC(dual=False, verbose=False, max_iter=1000)\n",
    "model.fit(\n",
    "    training_matrix,\n",
    "    [sentiments.name for sentiments in training_sentiments.values()]\n",
    ")\n",
    "svm_predictions = model.predict(test_matrix[0])\n",
    "\n",
    "svm_report = classification_report(\n",
    "    [sentiments.name for sentiments in test_sentiments[0].values()],\n",
    "    svm_predictions,\n",
    "    target_names=Sentiment.gts(),\n",
    "    digits=5\n",
    ")\n",
    "\n",
    "print(svm_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEAD CODE (too ugly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For features used for classifier training, the 'bow' feature is given in the code.\n",
    "# # But you could also explore the use of other features.\n",
    "# for classifier in ('svm', '<classifier-2-name>', '<classifier-3-name>',):\n",
    "#     for features in ('bow', '<feature-2-name>',):\n",
    "#         # Skeleton: Creation and training of the classifiers\n",
    "#         if classifier == 'svm':\n",
    "            \n",
    "#             print('Training ' + classifier)\n",
    "#         elif classifier == '<classifier-2-name>':\n",
    "#             # write the classifier 2 here\n",
    "#             print('Training ' + classifier)\n",
    "#         elif classifier == '<classifier-3-name>':\n",
    "#             # write the classifier 3 here\n",
    "#             print('Training ' + classifier)\n",
    "#         elif classifier == 'LSTM':\n",
    "#             # write the LSTM classifier here\n",
    "#             if features == 'bow':\n",
    "#                 continue\n",
    "#             print('Training ' + classifier)\n",
    "#         else:\n",
    "#             print('Unknown classifier name' + classifier)\n",
    "#             continue\n",
    "\n",
    "#         # Prediction performance of the classifiers\n",
    "#         for test_set_name in test_set_names:\n",
    "#             id_predicts = {}\n",
    "#             # write the prediction and evaluation code here\n",
    "#             evaluate(id_predicts, test_set_name, features + '-' + classifier)\n",
    "# custom vectorizer\n",
    "# BOW, unigram, bigram, trigram, n-gram (features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
