{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Two:  Sentiment Classification\n",
    "\n",
    "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
    "\n",
    "You are requested to produce a Jupyter notebook for the coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, don’t submit a Python program that takes as input some preprocessed files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from os import path, getcwd, listdir, makedirs\n",
    "\n",
    "\n",
    "def read_file_lines_from(file_path: str, /) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Read lines from a file and yield each line as a string.\n",
    "    The path to the file is relative to the current working directory.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be read.\n",
    "\n",
    "    Yields:\n",
    "        str: Each line of the file, stripped of leading and trailing whitespace.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), file_path)\n",
    "    buffer_size = 1024 * 1024\n",
    "    with open(full_path, 'r', buffering=buffer_size, encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            yield line.strip()\n",
    "\n",
    "\n",
    "def ls(dir_path: str, /) -> tuple[str, ...]:\n",
    "    \"\"\"\n",
    "    List all files and directories in the specified directory.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), dir_path)\n",
    "    return tuple(listdir(full_path))\n",
    "\n",
    "\n",
    "def mkdir(dir_path: str, /) -> None:\n",
    "    \"\"\"\n",
    "    Create a directory.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), dir_path)\n",
    "    if not path.exists(full_path):\n",
    "        makedirs(full_path)\n",
    "\n",
    "\n",
    "def path_exists(location: str, /) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the specified path exists.\n",
    "    \"\"\"\n",
    "    full_path = path.join(getcwd(), location)\n",
    "    return path.exists(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from threading import Thread, Lock\n",
    "from typing import final\n",
    "\n",
    "\n",
    "@final\n",
    "class BackgroundTask:\n",
    "    \"\"\"\n",
    "    Represents a background task that can be executed concurrently.\n",
    "\n",
    "    Args:\n",
    "        task (Callable): The function or method to be executed as a background task.\n",
    "        *args: Variable length argument list to be passed to the task.\n",
    "        **kwargs: Arbitrary keyword arguments to be passed to the task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task: Callable[..., None], *args, **kwargs):\n",
    "        self.__task = Thread(\n",
    "            target=task,\n",
    "            args=args,\n",
    "            kwargs=kwargs,\n",
    "            daemon=True\n",
    "        )\n",
    "        with Lock():\n",
    "            self.__task.start()\n",
    "\n",
    "    def wait(self) -> None:\n",
    "        \"\"\"\n",
    "        Waits for the background task to complete.\n",
    "        \"\"\"\n",
    "        return self.__task.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file_to(file_path: str, /, destination: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzip a file to a specified destination.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be unzipped.\n",
    "        destination (str): The path to the directory where the file will be unzipped.\n",
    "    \"\"\"\n",
    "    import zipfile\n",
    "    full_path = path.join(getcwd(), file_path)\n",
    "    with zipfile.ZipFile(full_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Final, Optional\n",
    "from shelve import open as shelve_open\n",
    "\n",
    "\n",
    "class GlobalCache:\n",
    "    \"\"\"\n",
    "    A simple global cache for storing data in memory.\n",
    "    \"\"\"\n",
    "\n",
    "    __runtime_cache: Final[dict[str, Any]] = {}\n",
    "    __cache_file_name: Final[str] = 'cache'\n",
    "\n",
    "    def put(self, key: str, value: object, /) -> None:\n",
    "        \"\"\"\n",
    "        Put a value into the cache.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key to be used to store the value.\n",
    "            value (object): The value to be stored.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__runtime_cache[key] = value\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            cache[key] = value\n",
    "\n",
    "    def get(self, key: str, /) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get a value from the cache.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key to be used to retrieve the value.\n",
    "\n",
    "        Returns:\n",
    "            object: The value stored in the cache.\n",
    "        \"\"\"\n",
    "\n",
    "        if key in self.__runtime_cache:\n",
    "            return self.__runtime_cache[key]\n",
    "\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            return cache.get(key)\n",
    "\n",
    "    def remove(self, key: str, /) -> None:\n",
    "        \"\"\"\n",
    "        Remove a value from the cache.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key to be used to remove the value.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__runtime_cache.pop(key, None)\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            del cache[key]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"\n",
    "        Clear the cache.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__runtime_cache.clear()\n",
    "        with shelve_open(self.__cache_file_name, 'c') as cache:\n",
    "            cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package imports for Application logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import regex as re\n",
    "import contractions\n",
    "import torch\n",
    "\n",
    "from os import cpu_count\n",
    "from typing import Final, final\n",
    "from types import NoneType\n",
    "from string import punctuation, digits\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from enum import Enum\n",
    "from collections import defaultdict, Counter\n",
    "from collections.abc import Sequence\n",
    "from copy import copy\n",
    "from huggingface_hub import hf_hub_download\n",
    "from emoji import EMOJI_DATA, demojize\n",
    "from nltk.downloader import download as nltk_download\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define global instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_path: Final[str] = 'data'\n",
    "glove_data_dir: Final[str] = f\"{dataset_base_path}/glove\"\n",
    "target_glove_file_name: Final[str] = \"glove.6B.100d.txt\"\n",
    "\n",
    "# names of the test set files\n",
    "test_set_names: Final[tuple[str, ...]] = (\n",
    "    'twitter-test1.txt',\n",
    "    'twitter-test2.txt',\n",
    "    'twitter-test3.txt',\n",
    ")\n",
    "training_data_file_name: Final[str] = 'twitter-training-data.txt'\n",
    "devlopment_data_file_name: Final[str] = 'twitter-dev-data.txt'\n",
    "\n",
    "\n",
    "@final\n",
    "class Sentiment(Enum):\n",
    "    \"\"\"\n",
    "    An enumeration of the three possible sentiment values.\n",
    "    \"\"\"\n",
    "    positive = 1\n",
    "    negative = -1\n",
    "    neutral = 0\n",
    "\n",
    "    @classmethod\n",
    "    @lru_cache\n",
    "    def gts(cls) -> tuple[str, ...]:\n",
    "        return tuple(cls.__members__.keys())\n",
    "\n",
    "\n",
    "global_cache = GlobalCache()\n",
    "\n",
    "TweetID = str\n",
    "ShouldMarkedAsBackground = NoneType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(typed=True)\n",
    "def get_tweets_from(file_name_: str, /) -> tuple[dict[TweetID, str], dict[TweetID, Sentiment]]:\n",
    "    \"\"\"\n",
    "    Read tweets from a file and return dictionaries containing tweet IDs, contents, and sentiments.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name_ (str): The name of the file to read tweets from.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing two dictionaries:\n",
    "        - id_gts (dict[TweetID, str]): A dictionary mapping tweet IDs to their contents.\n",
    "        - id_sentiments (dict[TweetID, Sentiment]): A dictionary mapping tweet IDs to their sentiments.\n",
    "    \"\"\"\n",
    "    id_gts: dict[TweetID, str] = {}\n",
    "    id_sentiments: dict[TweetID, Sentiment] = {}\n",
    "    lines = read_file_lines_from(f'{dataset_base_path}/{file_name_}')\n",
    "    for line in lines:\n",
    "        fields = line.split('\\t')\n",
    "        tweet_id = fields[0]\n",
    "        gt = fields[1]\n",
    "        content = ' '.join(fields[2:])\n",
    "        id_gts[tweet_id] = content\n",
    "        id_sentiments[tweet_id] = Sentiment[gt]\n",
    "\n",
    "    return id_gts, id_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define GloVe data preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_glove_data() -> ShouldMarkedAsBackground:\n",
    "    if path_exists(glove_data_dir) and len(ls(glove_data_dir)) == 4:\n",
    "        return\n",
    "\n",
    "    glove_data_pack_name = 'glove.6B.zip'\n",
    "\n",
    "    hf_hub_download(\n",
    "        repo_id='stanfordnlp/glove',\n",
    "        filename=glove_data_pack_name,\n",
    "        local_dir=dataset_base_path,\n",
    "        revision='1db2080b2d94def6e5b0386a523102f9d8849e9d',\n",
    "    )\n",
    "\n",
    "    # perform shell command using python code since the thread management can be done in python.\n",
    "    mkdir(glove_data_dir)\n",
    "    unzip_file_to(\n",
    "        f'{dataset_base_path}/{glove_data_pack_name}',\n",
    "        destination=glove_data_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(typed=True)\n",
    "def parse_glove_data(file_name_: str) -> tuple[dict[str, int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parse the GloVe data from a given file.\n",
    "\n",
    "    Args:\n",
    "        file_name_ (str): The name of the file containing the GloVe data.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[str, int], np.ndarray]: A tuple containing two elements:\n",
    "            - A dictionary mapping words to their corresponding indices.\n",
    "            - A numpy array containing the word vectors.\n",
    "    \"\"\"\n",
    "    file_frame = pd.read_csv(\n",
    "        f\"{glove_data_dir}/{file_name_}\",\n",
    "        delimiter=' ',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        encoding='utf-8',\n",
    "        skip_blank_lines=True,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        file_frame.reset_index().set_index(0)['index'].to_dict(),\n",
    "        file_frame.iloc[:, 1:].to_numpy(dtype=np.float64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(src: str, /, *, patterns: Sequence[re.Pattern]) -> str:\n",
    "    \"\"\"\n",
    "    Filters the given source text by removing all occurrences of the specified patterns.\n",
    "\n",
    "    Args:\n",
    "        src (str): The source text to be filtered.\n",
    "        patterns (Sequence[Pattern]): A sequence of regular expression patterns to be removed from the source text.\n",
    "\n",
    "    Returns:\n",
    "        str: The filtered text with all occurrences of the specified patterns removed.\n",
    "    \"\"\"\n",
    "    filtered = copy(src)\n",
    "\n",
    "    for pattern in patterns:\n",
    "        filtered = pattern.sub('', filtered)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def process_texts(src_dict: dict[Any, str], callable: Callable, *args, **kwargs) -> dict[Any, str]:\n",
    "    \"\"\"\n",
    "    Process a dictionary of texts using a callable function in parallel using a thread pool executor.\n",
    "\n",
    "    Args:\n",
    "        src_dict (dict[Any, str]): A dictionary containing the texts to be processed.\n",
    "        callable (Callable[[str], Any]): A callable function that will be applied to each text.\n",
    "        *args: Variable length argument list to be passed to the callable function.\n",
    "        **kargs: Arbitrary keyword arguments to be passed to the callable function.\n",
    "\n",
    "    Returns:\n",
    "        dict[Any, str]: A dictionary containing the processed texts.\n",
    "    \"\"\"\n",
    "\n",
    "    multi_process = False\n",
    "\n",
    "    if multi_process:\n",
    "        with ProcessPoolExecutor(max_workers=(cpu_count() or 1)) as executor:\n",
    "            future_to_key = {\n",
    "                executor.submit(callable, src_dict[key], *args, **kwargs): key for key in src_dict\n",
    "            }\n",
    "            return {\n",
    "                future_to_key[future]: future.result()\n",
    "                for future in as_completed(future_to_key)\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        key: callable(value, *args, **kwargs)\n",
    "        for key, value in src_dict.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def run_pipelines(\n",
    "    callables: Sequence[Callable[[str], str]],\n",
    "    /,\n",
    "    *,\n",
    "    tweets: dict[TweetID, str]\n",
    ") -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Run a sequence of callables on a dictionary of texts in parallel using a thread pool executor.\n",
    "\n",
    "    Args:\n",
    "        callables (Sequence[Callable[[dict[str, str]], dict[str, str]]]): A sequence of callable functions to be applied to the dictionary of texts.\n",
    "        tweets (dict[str, str]): A dictionary containing the texts to be processed.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, str]: A dictionary containing the processed texts.\n",
    "    \"\"\"\n",
    "    processed = copy(tweets)\n",
    "\n",
    "    for callable in callables:\n",
    "        processed = process_texts(processed, callable)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define confusion matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion(*, predict_results: dict[TweetID, Sentiment], test_set_file_name_: str) -> None:\n",
    "    \"\"\"\n",
    "    Display the confusion matrix based on the predicted results and the sentiment labels from the test set file.\n",
    "\n",
    "    Args:\n",
    "        predict_results (dict[TweetID, Sentiment]): A dictionary containing the predicted sentiment for each tweet ID.\n",
    "        test_set_file_name_ (str): The file name of the test set containing the sentiment labels for each tweet ID.\n",
    "    \"\"\"\n",
    "    _, id_sentiments = get_tweets_from(test_set_file_name_)\n",
    "\n",
    "    conf: Final[dict[Sentiment, dict[Sentiment, int]]] = defaultdict(\n",
    "        lambda: {\n",
    "            Sentiment.positive: 0,\n",
    "            Sentiment.negative: 0,\n",
    "            Sentiment.neutral: 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for tweet_id, sentiment in id_sentiments.items():\n",
    "        if tweet_id in predict_results:\n",
    "            pred = predict_results[tweet_id]\n",
    "        else:\n",
    "            pred = Sentiment.neutral\n",
    "        conf[pred][sentiment] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(Sentiment.gts()))\n",
    "\n",
    "    for c1 in Sentiment:\n",
    "        print(c1.name.ljust(12), end='')\n",
    "        for c2 in Sentiment:\n",
    "            if c1_sum := sum(conf[c1].values()) > 0:\n",
    "                p = conf[c1][c2] / float(c1_sum)\n",
    "                print(f\"{p:.3f}     \", end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict_results: dict[TweetID, Sentiment], test_set_file_name_: str, classifier_name_: str) -> None:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a sentiment classifier by comparing the predicted results with the ground truth sentiment labels.\n",
    "\n",
    "    Parameters:\n",
    "        - predict_results: A dictionary mapping TweetIDs to predicted Sentiments.\n",
    "        - test_set_file_name_: The name of the test set file.\n",
    "        - classifier_name_: The name of the classifier.\n",
    "    \"\"\"\n",
    "    _, id_sentiments = get_tweets_from(test_set_file_name_)\n",
    "\n",
    "    acc_by_class: Final[dict[Sentiment, dict[str, int]]] = defaultdict(\n",
    "        lambda: {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "    )\n",
    "\n",
    "    for tweet_id, sentiment in id_sentiments.items():\n",
    "        if tweet_id in predict_results:\n",
    "            pred = predict_results[tweet_id]\n",
    "        else:\n",
    "            pred = Sentiment.neutral\n",
    "\n",
    "        if sentiment == pred:\n",
    "            acc_by_class[sentiment]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[sentiment]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    cat_count = 0\n",
    "    item_count = 0\n",
    "    macro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "    micro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "    sem_eval_macro: dict[str, float] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
    "\n",
    "    micro_tp = 0.0\n",
    "    micro_fp = 0.0\n",
    "    micro_tn = 0.0\n",
    "    micro_fn = 0.0\n",
    "\n",
    "    cat_f1s: dict[Sentiment, float] = {}\n",
    "\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        cat_count += 1\n",
    "\n",
    "        micro_tp += acc['tp']\n",
    "        micro_fp += acc['fp']\n",
    "        micro_tn += acc['tn']\n",
    "        micro_fn += acc['fn']\n",
    "\n",
    "        p = 0.0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0.0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0.0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        cat_f1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            sem_eval_macro['p'] += p\n",
    "            sem_eval_macro['r'] += r\n",
    "            sem_eval_macro['f1'] += f1\n",
    "\n",
    "        item_count += n\n",
    "\n",
    "    micro['p'] = micro_tp / (micro_tp + micro_fp)\n",
    "    micro['r'] = micro_tp / (micro_tp + micro_fn)\n",
    "    micro['f1'] = 2 * micro['p'] * micro['r'] / (micro['p'] + micro['r'])\n",
    "\n",
    "    sem_eval_macro_f1 = sem_eval_macro['f1'] / 2\n",
    "\n",
    "    print(\n",
    "        f\"{test_set_file_name_} ({classifier_name_}): {sem_eval_macro_f1:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_sentiments = get_tweets_from(training_data_file_name)\n",
    "dev_data, dev_sentiments = get_tweets_from(devlopment_data_file_name)\n",
    "test_datas, test_sentiments = zip(*[\n",
    "    get_tweets_from(file_name)\n",
    "    for file_name in test_set_names\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download network resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_prepare_task = BackgroundTask(prepare_glove_data)\n",
    "\n",
    "\n",
    "def download_nltk_resources(resource_names: Sequence[str]) -> ShouldMarkedAsBackground:\n",
    "    for resource_name in resource_names:\n",
    "        nltk_download(resource_name, quiet=True)\n",
    "\n",
    "\n",
    "nltk_prepare_task = BackgroundTask(\n",
    "    download_nltk_resources,\n",
    "    ('stopwords', 'vader_lexicon', 'punkt', 'wordnet',)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all the words.\n",
    "\n",
    "def lowercase_tweet(tweet: str, /) -> str:\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the tweets based on the selected regexp patterns.\n",
    "\n",
    "re_flags = re.IGNORECASE | re.MULTILINE\n",
    "\n",
    "pattern_html_tags = re.compile(r'<[^>]+?>', re_flags)\n",
    "pattern_mentions = re.compile(r'@[a-zA-Z0-9_]+?', re_flags)\n",
    "pattern_hashtags = re.compile(r'#[a-zA-Z0-9_]+?', re_flags)\n",
    "pattern_alphanumeric = re.compile(r'[^a-zA-Z0-9 ]+?', re_flags)\n",
    "pattern_only_one_char = re.compile(r'\\b[a-zA-Z0-9]\\b', re_flags)\n",
    "pattern_fully_numeric = re.compile(r'\\b([0-9]+)\\b', re_flags)\n",
    "\n",
    "pattern_punctuation = re.compile(\n",
    "    \"[\" + re.escape(punctuation+\"“”…‘’\") + \"]+?\",\n",
    "    re_flags\n",
    ")\n",
    "\n",
    "pattern_url = re.compile(\n",
    "    r'(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?'\n",
    "    r'(//(?:[a-zA-Z0-9-._~%!$&\\'()*+,;=:]*(?::[a-zA-Z0-9-._~%!$&\\'()*+,;=:]+)?@)?'\n",
    "    r'(?:\\[[0-9a-fA-F:.]+]|(?:[a-zA-Z0-9-]+\\.)*[a-zA-Z]{2,}|[0-9.]+|localhost)'\n",
    "    r'(?::\\d+)?)(/[a-zA-Z0-9-._~%!$&\\'()*+,;=:@]*/?)*'\n",
    "    r'(?:\\?[a-zA-Z0-9-._~%!$&\\'()*+,;=:@/]*)?'\n",
    "    r'(?:#[a-zA-Z0-9-._~%!$&\\'()*+,;=:@/]*)?',\n",
    "    re_flags\n",
    ")\n",
    "\n",
    "all_emojis = tuple(EMOJI_DATA.keys())\n",
    "pattern_emojis = re.compile('|'.join(map(re.escape, all_emojis)) + '?')\n",
    "\n",
    "pattern_ampm = re.compile(r'([0-9]+(am|pm))')\n",
    "pattern_ordinals = re.compile(r'([0-9]+)(?:st|nd|rd|th)')\n",
    "\n",
    "selected_filter_patterns: tuple[re.Pattern[str], ...] = (\n",
    "    pattern_url,\n",
    "    pattern_html_tags,\n",
    "    pattern_mentions,\n",
    "    pattern_hashtags,\n",
    "    pattern_punctuation,\n",
    "    pattern_fully_numeric,\n",
    "    pattern_ampm,\n",
    "    pattern_ordinals,\n",
    ")\n",
    "\n",
    "\n",
    "def regexp_filter(tweet: str, /) -> str:\n",
    "    return filter_text(tweet, patterns=selected_filter_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(tweet: str, /) -> str:\n",
    "    return demojize(tweet, delimiters=('', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "\n",
    "\n",
    "def remove_numbers(tweet: str, /) -> str:\n",
    "    return tweet.translate(remove_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def remove_non_en(tweet: str, /) -> str:\n",
    "    try:\n",
    "        if detect(tweet) == 'en':\n",
    "            return tweet\n",
    "        else:\n",
    "            return ''\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_prepare_task.wait()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer(\n",
    "    reduce_len=True,\n",
    "    strip_handles=True,\n",
    "    preserve_case=False\n",
    ")\n",
    "stop_words = frozenset(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def nltk_tokenize_anti_stopwords_lemmatize(tweet: str, /) -> str:\n",
    "    wordnet.ensure_loaded()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    return ' '.join(\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_contractions(tweet: str, /) -> str:\n",
    "    return f'{contractions.fix(tweet)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run all preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pipelines = (\n",
    "    lowercase_tweet,\n",
    "    regexp_filter,\n",
    "    remove_emojis,\n",
    "    remove_numbers,\n",
    "    nltk_tokenize_anti_stopwords_lemmatize,\n",
    "    fix_contractions,\n",
    ")\n",
    "\n",
    "cleaned_training_tweets = run_pipelines(\n",
    "    all_pipelines,\n",
    "    tweets=training_data\n",
    ")\n",
    "\n",
    "cleaned_dev_tweets = run_pipelines(\n",
    "    all_pipelines,\n",
    "    tweets=dev_data\n",
    ")\n",
    "\n",
    "cleaned_test_tweets = [\n",
    "    run_pipelines(\n",
    "        all_pipelines,\n",
    "        tweets=test_data\n",
    "    )\n",
    "    for test_data in test_datas\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the top 10 most frequent words in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  freq\n",
      "0   tomorrow  5828\n",
      "1        may  5503\n",
      "2        not  3796\n",
      "3        day  3574\n",
      "4          i  3431\n",
      "5      going  3274\n",
      "6        you  3043\n",
      "7         am  2943\n",
      "8      night  2515\n",
      "9        see  2464\n",
      "10      like  2390\n",
      "11       get  2364\n",
      "12      time  2302\n",
      "13    sunday  2161\n",
      "14    friday  2075\n",
      "15       one  1937\n",
      "16        go  1860\n",
      "17      want  1846\n",
      "18       new  1692\n",
      "19      game  1631\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(cleaned_training_tweets.values()).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "freq_frame = pd.DataFrame(\n",
    "    word_freq.most_common(20),\n",
    "    columns=['word', 'freq']\n",
    ")\n",
    "\n",
    "print(freq_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['donated third time need keep one day uditthevote tillwithher',\n",
      " 'good luck pride parade tomorrow big city',\n",
      " 'join thousand christian people good september world day prayer god creation',\n",
      " 'bannon sure keen eye talent rump',\n",
      " 'jose mourinho want win playoff win tomorrow that is best alamadrid',\n",
      " 'google business workshop august sold do not worry book september workshop',\n",
      " 'mom stood congratulate nicki sat heard taylor lmao',\n",
      " 'realising sept today amp apple announcing new iphone ppleevent today made '\n",
      " 'day antwait xcited',\n",
      " 'gold standard terrible idea nail_polish',\n",
      " 'rt shpashh twilight win award tcas tonight burn twilight poster k ok',\n",
      " 'remember pain shadowy figure speculated narutos dad boom see true hokage '\n",
      " 'fight',\n",
      " 'paul dunne going first amateur since bobby jones win open pod going open '\n",
      " 'npodwetrust etherdunne',\n",
      " 'ussia especially interested coordinating ezbollahs infantry ground srael '\n",
      " 'yria',\n",
      " 'watching jurassic world literally time chris pratt',\n",
      " 'iranajam eid fall sept wed thur fri muharram thur fri october']\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Dev code.\n",
    "from pprint import pprint\n",
    "import random\n",
    "# generate a random number between 0 and 100\n",
    "\n",
    "window_size = 15\n",
    "rand_num = random.randint(0, len(cleaned_training_tweets) - window_size)\n",
    "\n",
    "pprint(list(cleaned_training_tweets.values())[rand_num:rand_num+window_size])\n",
    "print(rand_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45101, 2250)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    stop_words='english',\n",
    "    encoding='utf-8',\n",
    "    lowercase=False,\n",
    "    min_df=3,\n",
    "    max_df=0.8,\n",
    "    max_features=2250,\n",
    ")\n",
    "\n",
    "training_matrix = np.asarray(\n",
    "    csr_matrix(\n",
    "        vectorizer.fit_transform(cleaned_training_tweets.values())\n",
    "    ).todense()\n",
    ")\n",
    "dev_matrix = np.asarray(\n",
    "    csr_matrix(vectorizer.transform(cleaned_dev_tweets.values())).todense()\n",
    ")\n",
    "test_matrix = [\n",
    "    np.asarray(csr_matrix(vectorizer.transform(test_data.values())).todense())\n",
    "    for test_data in cleaned_test_tweets\n",
    "]\n",
    "\n",
    "print(training_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive    0.66390   0.28725   0.40100       557\n",
      "    negative    0.58202   0.83511   0.68596      1504\n",
      "     neutral    0.76148   0.58639   0.66257      1470\n",
      "\n",
      "    accuracy                        0.64514      3531\n",
      "   macro avg    0.66913   0.56958   0.58318      3531\n",
      "weighted avg    0.66965   0.64514   0.63127      3531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = LinearSVC(verbose=False, max_iter=1000, dual=False)\n",
    "model.fit(\n",
    "    training_matrix,\n",
    "    [sentiments.value for sentiments in training_sentiments.values()]\n",
    ")\n",
    "svm_predictions = model.predict(test_matrix[0])\n",
    "\n",
    "svm_report = classification_report(\n",
    "    [sentiments.value for sentiments in test_sentiments[0].values()],\n",
    "    svm_predictions,\n",
    "    target_names=Sentiment.gts(),\n",
    "    digits=5\n",
    ")\n",
    "\n",
    "print(svm_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GloVe Embedding Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_word_indexes: dict[str, int] = global_cache.get('glove_word_indexes') or {}\n",
    "glove_word_vectors: np.ndarray = global_cache.get('glove_word_vectors') or np.zeros(0)\n",
    "\n",
    "if len(glove_word_indexes) == 0 or len(glove_word_vectors) == 0:\n",
    "    glove_prepare_task.wait()\n",
    "    glove_word_indexes, glove_word_vectors = parse_glove_data(\n",
    "        target_glove_file_name\n",
    "    )\n",
    "    global_cache.put('glove_word_indexes', glove_word_indexes)\n",
    "    global_cache.put('glove_word_vectors', glove_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "@final\n",
    "class GloVeVectorizer:\n",
    "    def __init__(self, word_indexes: dict[str, int], word_vectors: np.ndarray):\n",
    "        self.word_indexes = word_indexes\n",
    "        self.word_vectors = word_vectors\n",
    "\n",
    "    def __vectorize_impl(self, tokens: Sequence[str], /) -> np.ndarray:\n",
    "        vecs = np.zeros(\n",
    "            (len(tokens), self.word_vectors.shape[1]),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in self.word_indexes:\n",
    "                vecs[i] = self.word_vectors[self.word_indexes[token]]\n",
    "        return vecs\n",
    "\n",
    "    @staticmethod\n",
    "    def __largest_element_count(element_sets: Sequence[Sequence | np.ndarray], /) -> int:\n",
    "        return max(\n",
    "            len(element)\n",
    "            for element in element_sets\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def __expand_to_size(\n",
    "        _2d_targets: Sequence[np.ndarray],\n",
    "        /,\n",
    "        *,\n",
    "        size: int\n",
    "    ) -> list[np.ndarray]:\n",
    "        assert size > 0\n",
    "        assert len(_2d_targets) > 0\n",
    "        return [\n",
    "            np.vstack((\n",
    "                _2d_target,\n",
    "                np.zeros(\n",
    "                    (size - _2d_target.shape[0], _2d_target.shape[1]),\n",
    "                    dtype=np.float64\n",
    "                )\n",
    "            ))\n",
    "            for _2d_target in _2d_targets\n",
    "        ]\n",
    "\n",
    "    def vectorize(self, tweets: dict[TweetID, str], /) -> np.ndarray:\n",
    "        tokens = [\n",
    "            self.__vectorize_impl(tweet.split())\n",
    "            for tweet in tweets.values()\n",
    "        ]\n",
    "        return np.asarray(\n",
    "            self.__expand_to_size(\n",
    "                tokens,\n",
    "                size=self.__largest_element_count(tokens)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45101\n"
     ]
    }
   ],
   "source": [
    "glove_vectorizer = GloVeVectorizer(glove_word_indexes, glove_word_vectors)\n",
    "\n",
    "training_glove_embeddings = glove_vectorizer.vectorize(cleaned_training_tweets)\n",
    "\n",
    "dev_glove_embeddings = glove_vectorizer.vectorize(cleaned_dev_tweets)\n",
    "\n",
    "test_glove_embeddings = [\n",
    "    glove_vectorizer.vectorize(test_data)\n",
    "    for test_data in cleaned_test_tweets\n",
    "]\n",
    "\n",
    "print(len(training_glove_embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
