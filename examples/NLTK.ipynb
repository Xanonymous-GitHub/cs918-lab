{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Getting Started\n",
    "\n",
    "### Chen Lyu - Chen.Lyu@warwick.ac.uk\n",
    "\n",
    "NLTK (Natural Language Tool Kit) is a Python module that provides easy-to-use interfaces to **over 50 corpora and lexical resources** such as *WordNet*, along with a suite of **text processing libraries** for *classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers* for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "NLTK [Documentation](https://www.nltk.org/)\n",
    "\n",
    "More information about the following exercises are available in [Chapter 1](http://www.nltk.org/book/ch01.html#sec-computing-with-language-texts-and-words) of the NLTK book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK text resources\n",
    "\n",
    "NLTK comes with a number of resoures. It is very handy to import and use them to build NLP tools. \n",
    "\n",
    "Let's start by listing NLTK resources available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# First, let's download NLTK corpora\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get \"permission denied\" error when running code in the cell above, try downloading by using the following command in a terminal instead:\n",
    "\n",
    "`sudo python -m nltk.downloader book`\n",
    "\n",
    "`sudo` provides root privileges when executing the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "# Print the list of the available books\n",
    "texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLTK Text object\n",
    "\n",
    "The Text object is a wrapper for a list of tokens representing the documents. \n",
    "\n",
    "Its methods perform a variety of analyses on the text’s contexts (e.g., counting, concordancing, collocation discovery), and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text1.tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance and similarity\n",
    "\n",
    "The NLTK `concordance()` function generates a list of all of the occurencies of a particular word within several contexts, showing how the word is being used. \n",
    "\n",
    "Let's try this on the Moby Dick text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that \"monstrous\" is often used in the context of size and whales. I guess this is no surprise given the book we're reading.\n",
    "\n",
    "Another function we can use here is the `similar()` function. This returns words which are used within similar contexts. \n",
    "E.g.: It looks for the words surrounding \"monstrous\" such as <i> \"most _ size\" </i> or <i>\"the _ pictures\"</i> and tries to find other words occuring in similar contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although perhaps a little tenuously related, these are all adjectives that do roughly fit the contexts described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Frequency Distributions and NLTK\n",
    "Now let's look at how to get how frequently the words are used in a corpus.\n",
    "\n",
    "NLTK provides a special `dictionary` that counts occurrences of items in a list. It is called `FreqDist` and allows you to plot graphs.\n",
    "\n",
    "Let's examine the words in Moby Dick with a frequency dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FreqDist(text1)\n",
    "\n",
    "print(\"--- Sample of word frequencies ---\")\n",
    "print(\"'the': \", f[\"the\"])\n",
    "print(\"'whale': \", f[\"whale\"])\n",
    "print(\"'monstrous': \", f[\"monstrous\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# draw the frequency of the 20 most common words\n",
    "f.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting but a lot of those being flagged up as the most frequent are common words like 'the', 'of', 'and', 'to'. \n",
    "\n",
    "These are what we call <b>stopwords</b> - words common to almost all documents and as such, that often provide **no value to an analyst**. We may want to filter these out. \n",
    "\n",
    "Thankfully NLTK comes with a stopwords list too. All we need to do is filter Moby Dick using this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as StopwordsLoader\n",
    "\n",
    "stopwords = StopwordsLoader.words() + [':','?','!','\"','--','-', \"'\", '.\"', ';','.',',']\n",
    "\n",
    "f = FreqDist([x for x in text1 if x.lower() not in stopwords]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and plot the most frequent words except stopwords\n",
    "print(f.most_common(20))\n",
    "\n",
    "f.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more interesting and informative. This plot helps painting the themes of the book. \n",
    "\n",
    "However, we can still observe a number of words that are not descriptive. Let's introduce a rule that filters out words shorter than 5 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FreqDist([x for x in text1 if (x.lower() not in stopwords and len(x) > 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and plot the most frequent words longer than 4 characters except stopwords\n",
    "print(f.most_common(20))\n",
    "\n",
    "f.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "Collocations are group of words that often occur together. For example, \"human beings\", \"The New York Times\" or \"emotional damage\". \n",
    "\n",
    "We find collocations by identifying the most frequent bigrams in the text. Bigrams are pairs of words that occur next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "print(list(bigrams(\"Moby Dick is about whales and human beings!\".split(\" \"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in collocations function calculates the most common bigrams in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collocations here are very specific to the book - Moby Dick. This gives us a great idea of the sorts of concepts and ideas that are important in Moby Dick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your own text with NLTK\n",
    "\n",
    "It's great that NLTK comes with so many resources, but how do you go about using your own corpus - If you have a series of plain text files, such as a movie review dataset?\n",
    "\n",
    "We use a `PlainTextCorpusReader` to enable NLTK to ingest and preprocess the corpus and allow us to do exercises like the ones above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possibile to create a Text object from a text file on your filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# Reading from disk and creating the Text object\n",
    "# PlaintextCorpusReader(root, fileids)\n",
    "my_local_corpus = PlaintextCorpusReader(\"Datasets/movie_reviews\", r\"\\w+\\.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that putting a letter \"r\" or \"R\" right before the string would turn it to a raw string object. Python raw string treats the backslash character as a literal character. Raw string is useful when a string needs to contain a backslash, such as for a regular expression or Windows directory path, and you don’t want it to be treated as an escape character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We have loaded the movie review corpus into NLTK, and now we can split it into words and sentences automatically.\n",
    "\n",
    "Let's examine the overall word frequency across the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FreqDist([x for x in my_local_corpus.words() if (x not in stopwords and len(x) > 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.most_common(20))\n",
    "\n",
    "f.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Not really any surprises here. Lots of words that make sense in a movie review context. Let's try doing collocations again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "my_corpus_text = Text(my_local_corpus.words())\n",
    "my_corpus_text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*BigramCollocationFinder* class unables us to apply more flexible operations with collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "# BigramCollocationFinder is a tool for the finding and ranking of bigram collocations or other association measures\n",
    "# It is often useful to use from_words() rather than constructing an instance directly.\n",
    "finder = BigramCollocationFinder.from_words(my_local_corpus.words())\n",
    "\n",
    "# Filter collocations appearing less than 3 times\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "# BigramAssocMeasures() returns a collection of Bigram association measures\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "# Pointwise Mutual Information (PMI) is a measurement of association\n",
    "# It compares the probability of two events occurring together to what this probability would be if the events were independent.\n",
    "# In NLP, it tells us how much more the two words co-occur in a corpus than we would have a priori expected them to appear by chance\n",
    "# PMI(x,y) = log(P(x,y)/(P(x)p(y)))\n",
    "print(finder.nbest(bigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more interesting. What we start to see are names of actors and other crew members from movies under review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading and more activities\n",
    "\n",
    "NLTK provides a huge amount of scope for NLP experiments and text mining. For more ideas and guidance it is worth reading the [NLTK book](http://www.nltk.org/book/) online.\n",
    "\n",
    "For an intuitive explanation of PMI:  https://stats.stackexchange.com/a/143150/83360"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
